{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150f5e0-8e61-4a4a-bc09-bb7fde5cc374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fbace-e1e2-4cba-8cfd-4f27041d02ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ml-collections==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c62e4-f717-4ce4-ac9a-598999b833a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ecbef4-2053-4dab-ac38-35d2e3c94f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from heroic_gm.models.heroic_nn import HeroicScoreNet\n",
    "from heroic_gm.data.heroic_dataset import HeroicDataset\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf8052-8f6b-44a4-b803-c0889016778b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SDE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2bba9-caf6-42f7-b5e5-6f475435ff2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Yang Song sde_lib class\n",
    "\"\"\"Abstract SDE classes, Reverse SDE, and VE/VP SDEs.\"\"\"\n",
    "import abc\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SDE(abc.ABC):\n",
    "  \"\"\"SDE abstract class. Functions are designed for a mini-batch of inputs.\"\"\"\n",
    "\n",
    "  def __init__(self, N):\n",
    "    \"\"\"Construct an SDE.\n",
    "\n",
    "    Args:\n",
    "      N: number of discretization time steps.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.N = N\n",
    "\n",
    "  @property\n",
    "  @abc.abstractmethod\n",
    "  def T(self):\n",
    "    \"\"\"End time of the SDE.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def sde(self, x, t):\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def marginal_prob(self, x, t):\n",
    "    \"\"\"Parameters to determine the marginal distribution of the SDE, $p_t(x)$.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def prior_sampling(self, shape):\n",
    "    \"\"\"Generate one sample from the prior distribution, $p_T(x)$.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def prior_logp(self, z):\n",
    "    \"\"\"Compute log-density of the prior distribution.\n",
    "\n",
    "    Useful for computing the log-likelihood via probability flow ODE.\n",
    "\n",
    "    Args:\n",
    "      z: latent code\n",
    "    Returns:\n",
    "      log probability density\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def discretize(self, x, t):\n",
    "    \"\"\"Discretize the SDE in the form: x_{i+1} = x_i + f_i(x_i) + G_i z_i.\n",
    "\n",
    "    Useful for reverse diffusion sampling and probabiliy flow sampling.\n",
    "    Defaults to Euler-Maruyama discretization.\n",
    "\n",
    "    Args:\n",
    "      x: a torch tensor\n",
    "      t: a torch float representing the time step (from 0 to `self.T`)\n",
    "\n",
    "    Returns:\n",
    "      f, G\n",
    "    \"\"\"\n",
    "    dt = 1 / self.N\n",
    "    drift, diffusion = self.sde(x, t)\n",
    "    f = drift * dt\n",
    "    G = diffusion * torch.sqrt(torch.tensor(dt, device=t.device))\n",
    "    return f, G\n",
    "\n",
    "  def reverse(self, score_fn, probability_flow=False):\n",
    "    \"\"\"Create the reverse-time SDE/ODE.\n",
    "\n",
    "    Args:\n",
    "      score_fn: A time-dependent score-based model that takes x and t and returns the score.\n",
    "      probability_flow: If `True`, create the reverse-time ODE used for probability flow sampling.\n",
    "    \"\"\"\n",
    "    N = self.N\n",
    "    T = self.T\n",
    "    sde_fn = self.sde\n",
    "    discretize_fn = self.discretize\n",
    "\n",
    "    # Build the class for reverse-time SDE.\n",
    "    class RSDE(self.__class__):\n",
    "      def __init__(self):\n",
    "        self.N = N\n",
    "        self.probability_flow = probability_flow\n",
    "\n",
    "      @property\n",
    "      def T(self):\n",
    "        return T\n",
    "\n",
    "      def sde(self, x, t):\n",
    "        \"\"\"Create the drift and diffusion functions for the reverse SDE/ODE.\"\"\"\n",
    "        drift, diffusion = sde_fn(x, t)\n",
    "        score = score_fn(x, t)\n",
    "        drift = drift - diffusion[:, None, None, None] ** 2 * score * (0.5 if self.probability_flow else 1.)\n",
    "        # Set the diffusion function to zero for ODEs.\n",
    "        diffusion = 0. if self.probability_flow else diffusion\n",
    "        return drift, diffusion\n",
    "\n",
    "      def discretize(self, x, t):\n",
    "        \"\"\"Create discretized iteration rules for the reverse diffusion sampler.\"\"\"\n",
    "        f, G = discretize_fn(x, t)\n",
    "        rev_f = f - G[:, None, None, None] ** 2 * score_fn(x, t) * (0.5 if self.probability_flow else 1.)\n",
    "        rev_G = torch.zeros_like(G) if self.probability_flow else G\n",
    "        return rev_f, rev_G\n",
    "\n",
    "    return RSDE()\n",
    "\n",
    "\n",
    "class VPSDE(SDE):\n",
    "  def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n",
    "    \"\"\"Construct a Variance Preserving SDE.\n",
    "\n",
    "    Args:\n",
    "      beta_min: value of beta(0)\n",
    "      beta_max: value of beta(1)\n",
    "      N: number of discretization steps\n",
    "    \"\"\"\n",
    "    super().__init__(N)\n",
    "    self.beta_0 = beta_min\n",
    "    self.beta_1 = beta_max\n",
    "    self.N = N\n",
    "    self.discrete_betas = torch.linspace(beta_min / N, beta_max / N, N)\n",
    "    self.alphas = 1. - self.discrete_betas\n",
    "    self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "    self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "    self.sqrt_1m_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "\n",
    "  @property\n",
    "  def T(self):\n",
    "    return 1\n",
    "\n",
    "  def sde(self, x, t):\n",
    "    beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n",
    "    drift = -0.5 * beta_t[:, None, None, None] * x\n",
    "    diffusion = torch.sqrt(beta_t)\n",
    "    return drift, diffusion\n",
    "\n",
    "  def marginal_prob(self, x, t):\n",
    "    log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n",
    "    mean = torch.exp(log_mean_coeff[:, None, None, None]) * x\n",
    "    std = torch.sqrt(1. - torch.exp(2. * log_mean_coeff))\n",
    "    return mean, std\n",
    "\n",
    "  def prior_sampling(self, shape):\n",
    "    return torch.randn(*shape)\n",
    "\n",
    "  def prior_logp(self, z):\n",
    "    shape = z.shape\n",
    "    N = np.prod(shape[1:])\n",
    "    logps = -N / 2. * np.log(2 * np.pi) - torch.sum(z ** 2, dim=(1, 2, 3)) / 2.\n",
    "    return logps\n",
    "\n",
    "  def discretize(self, x, t):\n",
    "    \"\"\"DDPM discretization.\"\"\"\n",
    "    timestep = (t * (self.N - 1) / self.T).long()\n",
    "    beta = self.discrete_betas.to(x.device)[timestep]\n",
    "    alpha = self.alphas.to(x.device)[timestep]\n",
    "    sqrt_beta = torch.sqrt(beta)\n",
    "    f = torch.sqrt(alpha)[:, None, None, None] * x - x\n",
    "    G = sqrt_beta\n",
    "    return f, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fa52b-108a-4141-ad20-6abd8d5f2711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_fn(model, train=False):\n",
    "  \"\"\"Create a function to give the output of the score-based model.\n",
    "\n",
    "  Args:\n",
    "    model: The score model.\n",
    "    train: `True` for training and `False` for evaluation.\n",
    "\n",
    "  Returns:\n",
    "    A model function.\n",
    "  \"\"\"\n",
    "\n",
    "  def model_fn(x, labels):\n",
    "    \"\"\"Compute the output of the score-based model.\n",
    "\n",
    "    Args:\n",
    "      x: A mini-batch of input data.\n",
    "      labels: A mini-batch of conditioning variables for time steps. Should be interpreted differently\n",
    "        for different models.\n",
    "\n",
    "    Returns:\n",
    "      A tuple of (model output, new mutable states)\n",
    "    \"\"\"\n",
    "    if not train:\n",
    "      model.eval()\n",
    "      return model(x, labels)\n",
    "    else:\n",
    "      model.train()\n",
    "      return model(x, labels)\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "\n",
    "def get_score_fn(sde, model, train=False, continuous=False):\n",
    "    \"\"\"Wraps `score_fn` so that the model output corresponds to a real time-dependent score function.\n",
    "\n",
    "    Args:\n",
    "    sde: An `sde_lib.SDE` object that represents the forward SDE.\n",
    "    model: A score model.\n",
    "    train: `True` for training and `False` for evaluation.\n",
    "    continuous: If `True`, the score-based model is expected to directly take continuous time steps.\n",
    "\n",
    "    Returns:\n",
    "    A score function.\n",
    "    \"\"\"\n",
    "    model_fn = get_model_fn(model, train=train)\n",
    "\n",
    "    def score_fn(x, t):\n",
    "        # For VP-trained models, t=0 corresponds to the lowest noise level\n",
    "        labels = t * (sde.N - 1)\n",
    "        score = model_fn(x, labels)\n",
    "        std = sde.sqrt_1m_alphas_cumprod.to(labels.device)[labels.long()]\n",
    "\n",
    "        score = -score / std[:, None, None, None]\n",
    "        return score\n",
    "\n",
    "    return score_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9aa75a-1cc8-43b5-8175-39d7be9a9194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ConditionalInstanceNorm2dPlus(nn.Module):\n",
    "  def __init__(self, num_features, num_classes, bias=True):\n",
    "    super().__init__()\n",
    "    self.num_features = num_features\n",
    "    self.bias = bias\n",
    "    self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n",
    "    if bias:\n",
    "      self.embed = nn.Embedding(num_classes, num_features * 3)\n",
    "      self.embed.weight.data[:, :2 * num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
    "      self.embed.weight.data[:, 2 * num_features:].zero_()  # Initialise bias at 0\n",
    "    else:\n",
    "      self.embed = nn.Embedding(num_classes, 2 * num_features)\n",
    "      self.embed.weight.data.normal_(1, 0.02)\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    means = torch.mean(x, dim=(2, 3))\n",
    "    m = torch.mean(means, dim=-1, keepdim=True)\n",
    "    v = torch.var(means, dim=-1, keepdim=True)\n",
    "    means = (means - m) / (torch.sqrt(v + 1e-5))\n",
    "    h = self.instance_norm(x)\n",
    "\n",
    "    if self.bias:\n",
    "      gamma, alpha, beta = self.embed(y).chunk(3, dim=-1)\n",
    "      h = h + means[..., None, None] * alpha[..., None, None]\n",
    "      out = gamma.view(-1, self.num_features, 1, 1) * h + beta.view(-1, self.num_features, 1, 1)\n",
    "    else:\n",
    "      gamma, alpha = self.embed(y).chunk(2, dim=-1)\n",
    "      h = h + means[..., None, None] * alpha[..., None, None]\n",
    "      out = gamma.view(-1, self.num_features, 1, 1) * h\n",
    "    return out\n",
    "\n",
    "def get_act(config):\n",
    "  \"\"\"Get activation functions from the config file.\"\"\"\n",
    "\n",
    "  if config.model.nonlinearity.lower() == 'elu':\n",
    "    return nn.ELU()\n",
    "  elif config.model.nonlinearity.lower() == 'relu':\n",
    "    return nn.ReLU()\n",
    "  elif config.model.nonlinearity.lower() == 'lrelu':\n",
    "    return nn.LeakyReLU(negative_slope=0.2)\n",
    "  elif config.model.nonlinearity.lower() == 'swish':\n",
    "    return nn.SiLU()\n",
    "  else:\n",
    "    raise NotImplementedError('activation function does not exist!')\n",
    "\n",
    "\n",
    "def ncsn_conv1x1(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=0):\n",
    "  \"\"\"1x1 convolution. Same as NCSNv1/v2.\"\"\"\n",
    "  conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias, dilation=dilation,\n",
    "                   padding=padding)\n",
    "  init_scale = 1e-10 if init_scale == 0 else init_scale\n",
    "  conv.weight.data *= init_scale\n",
    "  conv.bias.data *= init_scale\n",
    "  return conv\n",
    "\n",
    "\n",
    "def variance_scaling(scale, mode, distribution,\n",
    "                     in_axis=1, out_axis=0,\n",
    "                     dtype=torch.float32,\n",
    "                     device='cpu'):\n",
    "  \"\"\"Ported from JAX. \"\"\"\n",
    "\n",
    "  def _compute_fans(shape, in_axis=1, out_axis=0):\n",
    "    receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n",
    "    fan_in = shape[in_axis] * receptive_field_size\n",
    "    fan_out = shape[out_axis] * receptive_field_size\n",
    "    return fan_in, fan_out\n",
    "\n",
    "  def init(shape, dtype=dtype, device=device):\n",
    "    fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n",
    "    if mode == \"fan_in\":\n",
    "      denominator = fan_in\n",
    "    elif mode == \"fan_out\":\n",
    "      denominator = fan_out\n",
    "    elif mode == \"fan_avg\":\n",
    "      denominator = (fan_in + fan_out) / 2\n",
    "    else:\n",
    "      raise ValueError(\n",
    "        \"invalid mode for variance scaling initializer: {}\".format(mode))\n",
    "    variance = scale / denominator\n",
    "    if distribution == \"normal\":\n",
    "      return torch.randn(*shape, dtype=dtype, device=device) * np.sqrt(variance)\n",
    "    elif distribution == \"uniform\":\n",
    "      return (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n",
    "    else:\n",
    "      raise ValueError(\"invalid distribution for variance scaling initializer\")\n",
    "\n",
    "  return init\n",
    "\n",
    "\n",
    "def default_init(scale=1.):\n",
    "  \"\"\"The same initialization used in DDPM.\"\"\"\n",
    "  scale = 1e-10 if scale == 0 else scale\n",
    "  return variance_scaling(scale, 'fan_avg', 'uniform')\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "  \"\"\"Linear layer with `default_init`.\"\"\"\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "\n",
    "def ddpm_conv1x1(in_planes, out_planes, stride=1, bias=True, init_scale=1., padding=0):\n",
    "  \"\"\"1x1 convolution with DDPM initialization.\"\"\"\n",
    "  conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=padding, bias=bias)\n",
    "  conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n",
    "  nn.init.zeros_(conv.bias)\n",
    "  return conv\n",
    "\n",
    "\n",
    "def ncsn_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n",
    "  \"\"\"3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\"\"\"\n",
    "  init_scale = 1e-10 if init_scale == 0 else init_scale\n",
    "  conv = nn.Conv2d(in_planes, out_planes, stride=stride, bias=bias,\n",
    "                   dilation=dilation, padding=padding, kernel_size=3)\n",
    "  conv.weight.data *= init_scale\n",
    "  conv.bias.data *= init_scale\n",
    "  return conv\n",
    "\n",
    "\n",
    "def ddpm_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n",
    "  \"\"\"3x3 convolution with DDPM initialization.\"\"\"\n",
    "  conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding,\n",
    "                   dilation=dilation, bias=bias)\n",
    "  conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n",
    "  nn.init.zeros_(conv.bias)\n",
    "  return conv\n",
    "\n",
    "  ###########################################################################\n",
    "  # Functions below are ported over from the NCSNv1/NCSNv2 codebase:\n",
    "  # https://github.com/ermongroup/ncsn\n",
    "  # https://github.com/ermongroup/ncsnv2\n",
    "  ###########################################################################\n",
    "\n",
    "\n",
    "class CRPBlock(nn.Module):\n",
    "  def __init__(self, features, n_stages, act=nn.ReLU(), maxpool=True):\n",
    "    super().__init__()\n",
    "    self.convs = nn.ModuleList()\n",
    "    for i in range(n_stages):\n",
    "      self.convs.append(ncsn_conv3x3(features, features, stride=1, bias=False))\n",
    "    self.n_stages = n_stages\n",
    "    if maxpool:\n",
    "      self.pool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n",
    "    else:\n",
    "      self.pool = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "    self.act = act\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.act(x)\n",
    "    path = x\n",
    "    for i in range(self.n_stages):\n",
    "      path = self.pool(path)\n",
    "      path = self.convs[i](path)\n",
    "      x = path + x\n",
    "    return x\n",
    "\n",
    "\n",
    "class CondCRPBlock(nn.Module):\n",
    "  def __init__(self, features, n_stages, num_classes, normalizer, act=nn.ReLU()):\n",
    "    super().__init__()\n",
    "    self.convs = nn.ModuleList()\n",
    "    self.norms = nn.ModuleList()\n",
    "    self.normalizer = normalizer\n",
    "    for i in range(n_stages):\n",
    "      self.norms.append(normalizer(features, num_classes, bias=True))\n",
    "      self.convs.append(ncsn_conv3x3(features, features, stride=1, bias=False))\n",
    "\n",
    "    self.n_stages = n_stages\n",
    "    self.pool = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n",
    "    self.act = act\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    x = self.act(x)\n",
    "    path = x\n",
    "    for i in range(self.n_stages):\n",
    "      path = self.norms[i](path, y)\n",
    "      path = self.pool(path)\n",
    "      path = self.convs[i](path)\n",
    "\n",
    "      x = path + x\n",
    "    return x\n",
    "\n",
    "\n",
    "class RCUBlock(nn.Module):\n",
    "  def __init__(self, features, n_blocks, n_stages, act=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "      for j in range(n_stages):\n",
    "        setattr(self, '{}_{}_conv'.format(i + 1, j + 1), ncsn_conv3x3(features, features, stride=1, bias=False))\n",
    "\n",
    "    self.stride = 1\n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_stages = n_stages\n",
    "    self.act = act\n",
    "\n",
    "  def forward(self, x):\n",
    "    for i in range(self.n_blocks):\n",
    "      residual = x\n",
    "      for j in range(self.n_stages):\n",
    "        x = self.act(x)\n",
    "        x = getattr(self, '{}_{}_conv'.format(i + 1, j + 1))(x)\n",
    "\n",
    "      x += residual\n",
    "    return x\n",
    "\n",
    "\n",
    "class CondRCUBlock(nn.Module):\n",
    "  def __init__(self, features, n_blocks, n_stages, num_classes, normalizer, act=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "      for j in range(n_stages):\n",
    "        setattr(self, '{}_{}_norm'.format(i + 1, j + 1), normalizer(features, num_classes, bias=True))\n",
    "        setattr(self, '{}_{}_conv'.format(i + 1, j + 1), ncsn_conv3x3(features, features, stride=1, bias=False))\n",
    "\n",
    "    self.stride = 1\n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_stages = n_stages\n",
    "    self.act = act\n",
    "    self.normalizer = normalizer\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    for i in range(self.n_blocks):\n",
    "      residual = x\n",
    "      for j in range(self.n_stages):\n",
    "        x = getattr(self, '{}_{}_norm'.format(i + 1, j + 1))(x, y)\n",
    "        x = self.act(x)\n",
    "        x = getattr(self, '{}_{}_conv'.format(i + 1, j + 1))(x)\n",
    "\n",
    "      x += residual\n",
    "    return x\n",
    "\n",
    "\n",
    "class MSFBlock(nn.Module):\n",
    "  def __init__(self, in_planes, features):\n",
    "    super().__init__()\n",
    "    assert isinstance(in_planes, list) or isinstance(in_planes, tuple)\n",
    "    self.convs = nn.ModuleList()\n",
    "    self.features = features\n",
    "\n",
    "    for i in range(len(in_planes)):\n",
    "      self.convs.append(ncsn_conv3x3(in_planes[i], features, stride=1, bias=True))\n",
    "\n",
    "  def forward(self, xs, shape):\n",
    "    sums = torch.zeros(xs[0].shape[0], self.features, *shape, device=xs[0].device)\n",
    "    for i in range(len(self.convs)):\n",
    "      h = self.convs[i](xs[i])\n",
    "      h = F.interpolate(h, size=shape, mode='bilinear', align_corners=True)\n",
    "      sums += h\n",
    "    return sums\n",
    "\n",
    "\n",
    "class CondMSFBlock(nn.Module):\n",
    "  def __init__(self, in_planes, features, num_classes, normalizer):\n",
    "    super().__init__()\n",
    "    assert isinstance(in_planes, list) or isinstance(in_planes, tuple)\n",
    "\n",
    "    self.convs = nn.ModuleList()\n",
    "    self.norms = nn.ModuleList()\n",
    "    self.features = features\n",
    "    self.normalizer = normalizer\n",
    "\n",
    "    for i in range(len(in_planes)):\n",
    "      self.convs.append(ncsn_conv3x3(in_planes[i], features, stride=1, bias=True))\n",
    "      self.norms.append(normalizer(in_planes[i], num_classes, bias=True))\n",
    "\n",
    "  def forward(self, xs, y, shape):\n",
    "    sums = torch.zeros(xs[0].shape[0], self.features, *shape, device=xs[0].device)\n",
    "    for i in range(len(self.convs)):\n",
    "      h = self.norms[i](xs[i], y)\n",
    "      h = self.convs[i](h)\n",
    "      h = F.interpolate(h, size=shape, mode='bilinear', align_corners=True)\n",
    "      sums += h\n",
    "    return sums\n",
    "\n",
    "\n",
    "class RefineBlock(nn.Module):\n",
    "  def __init__(self, in_planes, features, act=nn.ReLU(), start=False, end=False, maxpool=True):\n",
    "    super().__init__()\n",
    "\n",
    "    assert isinstance(in_planes, tuple) or isinstance(in_planes, list)\n",
    "    self.n_blocks = n_blocks = len(in_planes)\n",
    "\n",
    "    self.adapt_convs = nn.ModuleList()\n",
    "    for i in range(n_blocks):\n",
    "      self.adapt_convs.append(RCUBlock(in_planes[i], 2, 2, act))\n",
    "\n",
    "    self.output_convs = RCUBlock(features, 3 if end else 1, 2, act)\n",
    "\n",
    "    if not start:\n",
    "      self.msf = MSFBlock(in_planes, features)\n",
    "\n",
    "    self.crp = CRPBlock(features, 2, act, maxpool=maxpool)\n",
    "\n",
    "  def forward(self, xs, output_shape):\n",
    "    assert isinstance(xs, tuple) or isinstance(xs, list)\n",
    "    hs = []\n",
    "    for i in range(len(xs)):\n",
    "      h = self.adapt_convs[i](xs[i])\n",
    "      hs.append(h)\n",
    "\n",
    "    if self.n_blocks > 1:\n",
    "      h = self.msf(hs, output_shape)\n",
    "    else:\n",
    "      h = hs[0]\n",
    "\n",
    "    h = self.crp(h)\n",
    "    h = self.output_convs(h)\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "class CondRefineBlock(nn.Module):\n",
    "  def __init__(self, in_planes, features, num_classes, normalizer, act=nn.ReLU(), start=False, end=False):\n",
    "    super().__init__()\n",
    "\n",
    "    assert isinstance(in_planes, tuple) or isinstance(in_planes, list)\n",
    "    self.n_blocks = n_blocks = len(in_planes)\n",
    "\n",
    "    self.adapt_convs = nn.ModuleList()\n",
    "    for i in range(n_blocks):\n",
    "      self.adapt_convs.append(\n",
    "        CondRCUBlock(in_planes[i], 2, 2, num_classes, normalizer, act)\n",
    "      )\n",
    "\n",
    "    self.output_convs = CondRCUBlock(features, 3 if end else 1, 2, num_classes, normalizer, act)\n",
    "\n",
    "    if not start:\n",
    "      self.msf = CondMSFBlock(in_planes, features, num_classes, normalizer)\n",
    "\n",
    "    self.crp = CondCRPBlock(features, 2, num_classes, normalizer, act)\n",
    "\n",
    "  def forward(self, xs, y, output_shape):\n",
    "    assert isinstance(xs, tuple) or isinstance(xs, list)\n",
    "    hs = []\n",
    "    for i in range(len(xs)):\n",
    "      h = self.adapt_convs[i](xs[i], y)\n",
    "      hs.append(h)\n",
    "\n",
    "    if self.n_blocks > 1:\n",
    "      h = self.msf(hs, y, output_shape)\n",
    "    else:\n",
    "      h = hs[0]\n",
    "\n",
    "    h = self.crp(h, y)\n",
    "    h = self.output_convs(h, y)\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "class ConvMeanPool(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True, adjust_padding=False):\n",
    "    super().__init__()\n",
    "    if not adjust_padding:\n",
    "      conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n",
    "      self.conv = conv\n",
    "    else:\n",
    "      conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n",
    "\n",
    "      self.conv = nn.Sequential(\n",
    "        nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "        conv\n",
    "      )\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    output = self.conv(inputs)\n",
    "    output = sum([output[:, :, ::2, ::2], output[:, :, 1::2, ::2],\n",
    "                  output[:, :, ::2, 1::2], output[:, :, 1::2, 1::2]]) / 4.\n",
    "    return output\n",
    "\n",
    "\n",
    "class MeanPoolConv(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    output = inputs\n",
    "    output = sum([output[:, :, ::2, ::2], output[:, :, 1::2, ::2],\n",
    "                  output[:, :, ::2, 1::2], output[:, :, 1::2, 1::2]]) / 4.\n",
    "    return self.conv(output)\n",
    "\n",
    "\n",
    "class UpsampleConv(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n",
    "    self.pixelshuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    output = inputs\n",
    "    output = torch.cat([output, output, output, output], dim=1)\n",
    "    output = self.pixelshuffle(output)\n",
    "    return self.conv(output)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, num_classes, resample=1, act=nn.ELU(),\n",
    "               normalization=ConditionalInstanceNorm2dPlus, adjust_padding=False, dilation=None):\n",
    "    super().__init__()\n",
    "    self.non_linearity = act\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.resample = resample\n",
    "    self.normalization = normalization\n",
    "    if resample == 'down':\n",
    "      if dilation > 1:\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, input_dim, dilation=dilation)\n",
    "        self.normalize2 = normalization(input_dim, num_classes)\n",
    "        self.conv2 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n",
    "        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n",
    "      else:\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, input_dim)\n",
    "        self.normalize2 = normalization(input_dim, num_classes)\n",
    "        self.conv2 = ConvMeanPool(input_dim, output_dim, 3, adjust_padding=adjust_padding)\n",
    "        conv_shortcut = partial(ConvMeanPool, kernel_size=1, adjust_padding=adjust_padding)\n",
    "\n",
    "    elif resample is None:\n",
    "      if dilation > 1:\n",
    "        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n",
    "        self.normalize2 = normalization(output_dim, num_classes)\n",
    "        self.conv2 = ncsn_conv3x3(output_dim, output_dim, dilation=dilation)\n",
    "      else:\n",
    "        conv_shortcut = nn.Conv2d\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, output_dim)\n",
    "        self.normalize2 = normalization(output_dim, num_classes)\n",
    "        self.conv2 = ncsn_conv3x3(output_dim, output_dim)\n",
    "    else:\n",
    "      raise Exception('invalid resample value')\n",
    "\n",
    "    if output_dim != input_dim or resample is not None:\n",
    "      self.shortcut = conv_shortcut(input_dim, output_dim)\n",
    "\n",
    "    self.normalize1 = normalization(input_dim, num_classes)\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    output = self.normalize1(x, y)\n",
    "    output = self.non_linearity(output)\n",
    "    output = self.conv1(output)\n",
    "    output = self.normalize2(output, y)\n",
    "    output = self.non_linearity(output)\n",
    "    output = self.conv2(output)\n",
    "\n",
    "    if self.output_dim == self.input_dim and self.resample is None:\n",
    "      shortcut = x\n",
    "    else:\n",
    "      shortcut = self.shortcut(x)\n",
    "\n",
    "    return shortcut + output\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, resample=None, act=nn.ELU(),\n",
    "               normalization=nn.InstanceNorm2d, adjust_padding=False, dilation=1):\n",
    "    super().__init__()\n",
    "    self.non_linearity = act\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.resample = resample\n",
    "    self.normalization = normalization\n",
    "    if resample == 'down':\n",
    "      if dilation > 1:\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, input_dim, dilation=dilation)\n",
    "        self.normalize2 = normalization(input_dim)\n",
    "        self.conv2 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n",
    "        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n",
    "      else:\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, input_dim)\n",
    "        self.normalize2 = normalization(input_dim)\n",
    "        self.conv2 = ConvMeanPool(input_dim, output_dim, 3, adjust_padding=adjust_padding)\n",
    "        conv_shortcut = partial(ConvMeanPool, kernel_size=1, adjust_padding=adjust_padding)\n",
    "\n",
    "    elif resample is None:\n",
    "      if dilation > 1:\n",
    "        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n",
    "        self.normalize2 = normalization(output_dim)\n",
    "        self.conv2 = ncsn_conv3x3(output_dim, output_dim, dilation=dilation)\n",
    "      else:\n",
    "        # conv_shortcut = nn.Conv2d ### Something wierd here.\n",
    "        conv_shortcut = partial(ncsn_conv1x1)\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, output_dim)\n",
    "        self.normalize2 = normalization(output_dim)\n",
    "        self.conv2 = ncsn_conv3x3(output_dim, output_dim)\n",
    "    else:\n",
    "      raise Exception('invalid resample value')\n",
    "\n",
    "    if output_dim != input_dim or resample is not None:\n",
    "      self.shortcut = conv_shortcut(input_dim, output_dim)\n",
    "\n",
    "    self.normalize1 = normalization(input_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.normalize1(x)\n",
    "    output = self.non_linearity(output)\n",
    "    output = self.conv1(output)\n",
    "    output = self.normalize2(output)\n",
    "    output = self.non_linearity(output)\n",
    "    output = self.conv2(output)\n",
    "\n",
    "    if self.output_dim == self.input_dim and self.resample is None:\n",
    "      shortcut = x\n",
    "    else:\n",
    "      shortcut = self.shortcut(x)\n",
    "\n",
    "    return shortcut + output\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# Functions below are ported over from the DDPM codebase:\n",
    "#  https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py\n",
    "###########################################################################\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n",
    "  assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n",
    "  half_dim = embedding_dim // 2\n",
    "  # magic number 10000 is from transformers\n",
    "  emb = math.log(max_positions) / (half_dim - 1)\n",
    "  # emb = math.log(2.) / (half_dim - 1)\n",
    "  emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "  # emb = tf.range(num_embeddings, dtype=jnp.float32)[:, None] * emb[None, :]\n",
    "  # emb = tf.cast(timesteps, dtype=jnp.float32)[:, None] * emb[None, :]\n",
    "  emb = timesteps.float()[:, None] * emb[None, :]\n",
    "  emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "  if embedding_dim % 2 == 1:  # zero pad\n",
    "    emb = F.pad(emb, (0, 1), mode='constant')\n",
    "  assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "  return emb\n",
    "\n",
    "\n",
    "def _einsum(a, b, c, x, y):\n",
    "  einsum_str = '{},{}->{}'.format(''.join(a), ''.join(b), ''.join(c))\n",
    "  return torch.einsum(einsum_str, x, y)\n",
    "\n",
    "\n",
    "def contract_inner(x, y):\n",
    "  \"\"\"tensordot(x, y, 1).\"\"\"\n",
    "  x_chars = list(string.ascii_lowercase[:len(x.shape)])\n",
    "  y_chars = list(string.ascii_lowercase[len(x.shape):len(y.shape) + len(x.shape)])\n",
    "  y_chars[0] = x_chars[-1]  # first axis of y and last of x get summed\n",
    "  out_chars = x_chars[:-1] + y_chars[1:]\n",
    "  return _einsum(x_chars, y_chars, out_chars, x, y)\n",
    "\n",
    "\n",
    "class NIN(nn.Module):\n",
    "  def __init__(self, in_dim, num_units, init_scale=0.1):\n",
    "    super().__init__()\n",
    "    self.W = nn.Parameter(default_init(scale=init_scale)((in_dim, num_units)), requires_grad=True)\n",
    "    self.b = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    y = contract_inner(x, self.W) + self.b\n",
    "    return y.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "  \"\"\"Channel-wise self-attention block.\"\"\"\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n",
    "    self.NIN_0 = NIN(channels, channels)\n",
    "    self.NIN_1 = NIN(channels, channels)\n",
    "    self.NIN_2 = NIN(channels, channels)\n",
    "    self.NIN_3 = NIN(channels, channels, init_scale=0.)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, C, H, W = x.shape\n",
    "    h = self.GroupNorm_0(x)\n",
    "    q = self.NIN_0(h)\n",
    "    k = self.NIN_1(h)\n",
    "    v = self.NIN_2(h)\n",
    "\n",
    "    w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n",
    "    w = torch.reshape(w, (B, H, W, H * W))\n",
    "    w = F.softmax(w, dim=-1)\n",
    "    w = torch.reshape(w, (B, H, W, H, W))\n",
    "    h = torch.einsum('bhwij,bcij->bchw', w, v)\n",
    "    h = self.NIN_3(h)\n",
    "    return x + h\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "  def __init__(self, channels, with_conv=False):\n",
    "    super().__init__()\n",
    "    if with_conv:\n",
    "      self.Conv_0 = ddpm_conv3x3(channels, channels)\n",
    "    self.with_conv = with_conv\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, C, H, W = x.shape\n",
    "    h = F.interpolate(x, (H * 2, W * 2), mode='nearest')\n",
    "    if self.with_conv:\n",
    "      h = self.Conv_0(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "  def __init__(self, channels, with_conv=False):\n",
    "    super().__init__()\n",
    "    if with_conv:\n",
    "      self.Conv_0 = ddpm_conv3x3(channels, channels, stride=2, padding=0)\n",
    "    self.with_conv = with_conv\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, C, H, W = x.shape\n",
    "    # Emulate 'SAME' padding\n",
    "    if self.with_conv:\n",
    "      x = F.pad(x, (0, 1, 0, 1))\n",
    "      x = self.Conv_0(x)\n",
    "    else:\n",
    "      x = F.avg_pool2d(x, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    assert x.shape == (B, C, H // 2, W // 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "class ResnetBlockDDPM(nn.Module):\n",
    "  \"\"\"The ResNet Blocks used in DDPM.\"\"\"\n",
    "  def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1):\n",
    "    super().__init__()\n",
    "    if out_ch is None:\n",
    "      out_ch = in_ch\n",
    "    self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=in_ch, eps=1e-6)\n",
    "    self.act = act\n",
    "    self.Conv_0 = ddpm_conv3x3(in_ch, out_ch)\n",
    "    if temb_dim is not None:\n",
    "      self.Dense_0 = nn.Linear(temb_dim, out_ch)\n",
    "      self.Dense_0.weight.data = default_init()(self.Dense_0.weight.data.shape)\n",
    "      nn.init.zeros_(self.Dense_0.bias)\n",
    "\n",
    "    self.GroupNorm_1 = nn.GroupNorm(num_groups=32, num_channels=out_ch, eps=1e-6)\n",
    "    self.Dropout_0 = nn.Dropout(dropout)\n",
    "    self.Conv_1 = ddpm_conv3x3(out_ch, out_ch, init_scale=0.)\n",
    "    if in_ch != out_ch:\n",
    "      if conv_shortcut:\n",
    "        self.Conv_2 = ddpm_conv3x3(in_ch, out_ch)\n",
    "      else:\n",
    "        self.NIN_0 = NIN(in_ch, out_ch)\n",
    "    self.out_ch = out_ch\n",
    "    self.in_ch = in_ch\n",
    "    self.conv_shortcut = conv_shortcut\n",
    "\n",
    "  def forward(self, x, temb=None):\n",
    "    B, C, H, W = x.shape\n",
    "    assert C == self.in_ch\n",
    "    out_ch = self.out_ch if self.out_ch else self.in_ch\n",
    "    h = self.act(self.GroupNorm_0(x))\n",
    "    h = self.Conv_0(h)\n",
    "    # Add bias to each feature map conditioned on the time embedding\n",
    "    if temb is not None:\n",
    "      h += self.Dense_0(self.act(temb))[:, :, None, None]\n",
    "    h = self.act(self.GroupNorm_1(h))\n",
    "    h = self.Dropout_0(h)\n",
    "    h = self.Conv_1(h)\n",
    "    if C != out_ch:\n",
    "      if self.conv_shortcut:\n",
    "        x = self.Conv_2(x)\n",
    "      else:\n",
    "        x = self.NIN_0(x)\n",
    "    return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde56164-63ad-4e66-9c07-564f9063a703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "\n",
    "def get_normalization(config, conditional=False):\n",
    "  \"\"\"Obtain normalization modules from the config file.\"\"\"\n",
    "  norm = config.model.normalization\n",
    "  if conditional:\n",
    "    if norm == 'InstanceNorm++':\n",
    "      return functools.partial(ConditionalInstanceNorm2dPlus, num_classes=config.model.num_classes)\n",
    "    else:\n",
    "      raise NotImplementedError(f'{norm} not implemented yet.')\n",
    "  else:\n",
    "    if norm == 'InstanceNorm':\n",
    "      return nn.InstanceNorm2d\n",
    "    elif norm == 'InstanceNorm++':\n",
    "      return InstanceNorm2dPlus\n",
    "    elif norm == 'VarianceNorm':\n",
    "      return VarianceNorm2d\n",
    "    elif norm == 'GroupNorm':\n",
    "      return nn.GroupNorm\n",
    "    else:\n",
    "      raise ValueError('Unknown normalization: %s' % norm)\n",
    "\n",
    "def variance_scaling(scale, mode, distribution,\n",
    "                     in_axis=1, out_axis=0,\n",
    "                     dtype=torch.float32,\n",
    "                     device='cpu'):\n",
    "  \"\"\"Ported from JAX. \"\"\"\n",
    "\n",
    "  def _compute_fans(shape, in_axis=1, out_axis=0):\n",
    "    receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n",
    "    fan_in = shape[in_axis] * receptive_field_size\n",
    "    fan_out = shape[out_axis] * receptive_field_size\n",
    "    return fan_in, fan_out\n",
    "\n",
    "  def init(shape, dtype=dtype, device=device):\n",
    "    fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n",
    "    if mode == \"fan_in\":\n",
    "      denominator = fan_in\n",
    "    elif mode == \"fan_out\":\n",
    "      denominator = fan_out\n",
    "    elif mode == \"fan_avg\":\n",
    "      denominator = (fan_in + fan_out) / 2\n",
    "    else:\n",
    "      raise ValueError(\n",
    "        \"invalid mode for variance scaling initializer: {}\".format(mode))\n",
    "    variance = scale / denominator\n",
    "    if distribution == \"normal\":\n",
    "      return torch.randn(*shape, dtype=dtype, device=device) * np.sqrt(variance)\n",
    "    elif distribution == \"uniform\":\n",
    "      return (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n",
    "    else:\n",
    "      raise ValueError(\"invalid distribution for variance scaling initializer\")\n",
    "\n",
    "  return init\n",
    "\n",
    "\n",
    "def default_init(scale=1.):\n",
    "  \"\"\"The same initialization used in DDPM.\"\"\"\n",
    "  scale = 1e-10 if scale == 0 else scale\n",
    "  return variance_scaling(scale, 'fan_avg', 'uniform')\n",
    "\n",
    "default_initializer = default_init\n",
    "conv3x3 = ddpm_conv3x3\n",
    "\n",
    "def get_sigmas(config):\n",
    "  \"\"\"Get sigmas --- the set of noise levels for SMLD from config files.\n",
    "  Args:\n",
    "    config: A ConfigDict object parsed from the config file\n",
    "  Returns:\n",
    "    sigmas: a jax numpy arrary of noise levels\n",
    "  \"\"\"\n",
    "  sigmas = np.exp(\n",
    "    np.linspace(np.log(config.model.sigma_max), np.log(config.model.sigma_min), config.model.num_scales))\n",
    "\n",
    "  return sigmas\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n",
    "  assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n",
    "  half_dim = embedding_dim // 2\n",
    "  # magic number 10000 is from transformers\n",
    "  emb = math.log(max_positions) / (half_dim - 1)\n",
    "  # emb = math.log(2.) / (half_dim - 1)\n",
    "  emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "  # emb = tf.range(num_embeddings, dtype=jnp.float32)[:, None] * emb[None, :]\n",
    "  # emb = tf.cast(timesteps, dtype=jnp.float32)[:, None] * emb[None, :]\n",
    "  emb = timesteps.float()[:, None] * emb[None, :]\n",
    "  emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "  if embedding_dim % 2 == 1:  # zero pad\n",
    "    emb = F.pad(emb, (0, 1), mode='constant')\n",
    "  assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "  return emb\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.act = act = get_act(config)\n",
    "    self.register_buffer('sigmas', torch.tensor(get_sigmas(config)))\n",
    "\n",
    "    self.nf = nf = config.model.nf\n",
    "    ch_mult = config.model.ch_mult\n",
    "    self.num_res_blocks = num_res_blocks = config.model.num_res_blocks\n",
    "    self.attn_resolutions = attn_resolutions = config.model.attn_resolutions\n",
    "    dropout = config.model.dropout\n",
    "    resamp_with_conv = config.model.resamp_with_conv\n",
    "    self.num_resolutions = num_resolutions = len(ch_mult)\n",
    "    self.all_resolutions = all_resolutions = [config.data.image_size // (2 ** i) for i in range(num_resolutions)]\n",
    "\n",
    "    #AttnBlock = functools.partial(AttnBlock)\n",
    "    self.conditional = conditional = config.model.conditional\n",
    "    ResnetBlock = functools.partial(ResnetBlockDDPM, act=act, temb_dim=4 * nf, dropout=dropout)\n",
    "    if conditional:\n",
    "      # Condition on noise levels.\n",
    "      modules = [nn.Linear(nf, nf * 4)]\n",
    "      modules[0].weight.data = default_initializer()(modules[0].weight.data.shape)\n",
    "      nn.init.zeros_(modules[0].bias)\n",
    "      modules.append(nn.Linear(nf * 4, nf * 4))\n",
    "      modules[1].weight.data = default_initializer()(modules[1].weight.data.shape)\n",
    "      nn.init.zeros_(modules[1].bias)\n",
    "\n",
    "    self.centered = config.data.centered\n",
    "    channels = config.data.num_channels\n",
    "\n",
    "    # Downsampling block\n",
    "    modules.append(conv3x3(channels, nf))\n",
    "    hs_c = [nf]\n",
    "    in_ch = nf\n",
    "    for i_level in range(num_resolutions):\n",
    "      # Residual blocks for this resolution\n",
    "      for i_block in range(num_res_blocks):\n",
    "        out_ch = nf * ch_mult[i_level]\n",
    "        modules.append(ResnetBlock(in_ch=in_ch, out_ch=out_ch))\n",
    "        in_ch = out_ch\n",
    "        if all_resolutions[i_level] in attn_resolutions:\n",
    "          modules.append(AttnBlock(channels=in_ch))\n",
    "        hs_c.append(in_ch)\n",
    "      if i_level != num_resolutions - 1:\n",
    "        modules.append(Downsample(channels=in_ch, with_conv=resamp_with_conv))\n",
    "        hs_c.append(in_ch)\n",
    "\n",
    "    in_ch = hs_c[-1]\n",
    "    modules.append(ResnetBlock(in_ch=in_ch))\n",
    "    modules.append(AttnBlock(channels=in_ch))\n",
    "    modules.append(ResnetBlock(in_ch=in_ch))\n",
    "\n",
    "    # Upsampling block\n",
    "    for i_level in reversed(range(num_resolutions)):\n",
    "      for i_block in range(num_res_blocks + 1):\n",
    "        out_ch = nf * ch_mult[i_level]\n",
    "        modules.append(ResnetBlock(in_ch=in_ch + hs_c.pop(), out_ch=out_ch))\n",
    "        in_ch = out_ch\n",
    "      if all_resolutions[i_level] in attn_resolutions:\n",
    "        modules.append(AttnBlock(channels=in_ch))\n",
    "      if i_level != 0:\n",
    "        modules.append(Upsample(channels=in_ch, with_conv=resamp_with_conv))\n",
    "\n",
    "    assert not hs_c\n",
    "    modules.append(nn.GroupNorm(num_channels=in_ch, num_groups=32, eps=1e-6))\n",
    "    modules.append(conv3x3(in_ch, channels, init_scale=0.))\n",
    "    self.all_modules = nn.ModuleList(modules)\n",
    "\n",
    "    self.scale_by_sigma = config.model.scale_by_sigma\n",
    "\n",
    "  def forward(self, x, labels):\n",
    "    modules = self.all_modules\n",
    "    m_idx = 0\n",
    "    if self.conditional:\n",
    "      # timestep/scale embedding\n",
    "      timesteps = labels\n",
    "      temb = get_timestep_embedding(timesteps, self.nf)\n",
    "      temb = modules[m_idx](temb)\n",
    "      m_idx += 1\n",
    "      temb = modules[m_idx](self.act(temb))\n",
    "      m_idx += 1\n",
    "    else:\n",
    "      temb = None\n",
    "\n",
    "    if self.centered:\n",
    "      # Input is in [-1, 1]\n",
    "      h = x\n",
    "    else:\n",
    "      # Input is in [0, 1]\n",
    "      h = 2 * x - 1.\n",
    "\n",
    "    # Downsampling block\n",
    "    hs = [modules[m_idx](h)]\n",
    "    m_idx += 1\n",
    "    for i_level in range(self.num_resolutions):\n",
    "      # Residual blocks for this resolution\n",
    "      for i_block in range(self.num_res_blocks):\n",
    "        h = modules[m_idx](hs[-1], temb)\n",
    "        m_idx += 1\n",
    "        if h.shape[-1] in self.attn_resolutions:\n",
    "          h = modules[m_idx](h)\n",
    "          m_idx += 1\n",
    "        hs.append(h)\n",
    "      if i_level != self.num_resolutions - 1:\n",
    "        hs.append(modules[m_idx](hs[-1]))\n",
    "        m_idx += 1\n",
    "\n",
    "    h = hs[-1]\n",
    "    h = modules[m_idx](h, temb)\n",
    "    m_idx += 1\n",
    "    h = modules[m_idx](h)\n",
    "    m_idx += 1\n",
    "    h = modules[m_idx](h, temb)\n",
    "    m_idx += 1\n",
    "\n",
    "    # Upsampling block\n",
    "    for i_level in reversed(range(self.num_resolutions)):\n",
    "      for i_block in range(self.num_res_blocks + 1):\n",
    "        h = modules[m_idx](torch.cat([h, hs.pop()], dim=1), temb)\n",
    "        m_idx += 1\n",
    "      if h.shape[-1] in self.attn_resolutions:\n",
    "        h = modules[m_idx](h)\n",
    "        m_idx += 1\n",
    "      if i_level != 0:\n",
    "        h = modules[m_idx](h)\n",
    "        m_idx += 1\n",
    "\n",
    "    assert not hs\n",
    "    h = self.act(modules[m_idx](h))\n",
    "    m_idx += 1\n",
    "    h = modules[m_idx](h)\n",
    "    m_idx += 1\n",
    "    assert m_idx == len(modules)\n",
    "\n",
    "    if self.scale_by_sigma:\n",
    "      # Divide the output by sigmas. Useful for training with the NCSN loss.\n",
    "      # The DDPM loss scales the network output by sigma in the loss function,\n",
    "      # so no need of doing it here.\n",
    "      used_sigmas = self.sigmas[labels, None, None, None]\n",
    "      h = h / used_sigmas\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10462656-116c-4129-9753-e6a42397bac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_default_configs():\n",
    "  config = ml_collections.ConfigDict()\n",
    "  # training\n",
    "  config.training = training = ml_collections.ConfigDict()\n",
    "  config.training.batch_size = 50\n",
    "  training.n_iters = 1300001\n",
    "  training.snapshot_freq = 50000\n",
    "  training.log_freq = 50\n",
    "  training.eval_freq = 100\n",
    "  ## store additional checkpoints for preemption in cloud computing environments\n",
    "  training.snapshot_freq_for_preemption = 10000\n",
    "  ## produce samples at each snapshot.\n",
    "  training.snapshot_sampling = True\n",
    "  training.likelihood_weighting = False\n",
    "  training.continuous = False\n",
    "  training.reduce_mean = False\n",
    "\n",
    "  # sampling\n",
    "  config.sampling = sampling = ml_collections.ConfigDict()\n",
    "  sampling.n_steps_each = 1\n",
    "  sampling.noise_removal = True\n",
    "  sampling.probability_flow = False\n",
    "  sampling.snr = 0.17\n",
    "  sampling.method = 'pc'\n",
    "\n",
    "  # evaluation\n",
    "  config.eval = evaluate = ml_collections.ConfigDict()\n",
    "  evaluate.begin_ckpt = 1\n",
    "  evaluate.end_ckpt = 26\n",
    "  evaluate.batch_size = 1024\n",
    "  evaluate.enable_sampling = True\n",
    "  evaluate.num_samples = 50000\n",
    "  evaluate.enable_loss = True\n",
    "  evaluate.enable_bpd = False\n",
    "  evaluate.bpd_dataset = 'test'\n",
    "\n",
    "  # data\n",
    "  config.data = data = ml_collections.ConfigDict()\n",
    "  data.dataset = 'DungeonDiffusion'\n",
    "  data.image_size = 24\n",
    "  data.random_flip = True\n",
    "  data.uniform_dequantization = False\n",
    "  data.centered = False\n",
    "  data.num_channels = 3\n",
    "\n",
    "  # model\n",
    "  config.model = model = ml_collections.ConfigDict()\n",
    "  model.sigma_max = 90.\n",
    "  model.sigma_min = 0.01\n",
    "  model.num_scales = 1000\n",
    "  model.beta_min = 0.1\n",
    "  model.beta_max = 20.\n",
    "  model.dropout = 0.1\n",
    "  model.embedding_type = 'fourier'\n",
    "  model.nonlinearity = 'elu'\n",
    "  model.nf = 128\n",
    "  model.ch_mult = (1, 2, 2, 2)\n",
    "  model.num_res_blocks = 2\n",
    "  model.attn_resolutions = (16,)\n",
    "  model.resamp_with_conv = True\n",
    "  model.conditional = True\n",
    "  model.scale_by_sigma = True\n",
    "\n",
    "  # optimization\n",
    "  config.optim = optim = ml_collections.ConfigDict()\n",
    "  optim.weight_decay = 0\n",
    "  optim.optimizer = 'Adam'\n",
    "  optim.lr = 2e-4\n",
    "  optim.beta1 = 0.9\n",
    "  optim.eps = 1e-8\n",
    "  optim.warmup = 5000\n",
    "  optim.grad_clip = 1.\n",
    "\n",
    "  config.seed = 42\n",
    "  config.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "  return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9cfc4-077b-4e59-a69e-cff5e869f6a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae626c8-620a-4c74-b54d-4b56167b7306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def get_model_fn(model, train=False):\n",
    "  \"\"\"Create a function to give the output of the score-based model.\n",
    "\n",
    "  Args:\n",
    "    model: The score model.\n",
    "    train: `True` for training and `False` for evaluation.\n",
    "\n",
    "  Returns:\n",
    "    A model function.\n",
    "  \"\"\"\n",
    "\n",
    "  def model_fn(x, labels):\n",
    "    \"\"\"Compute the output of the score-based model.\n",
    "\n",
    "    Args:\n",
    "      x: A mini-batch of input data.\n",
    "      labels: A mini-batch of conditioning variables for time steps. Should be interpreted differently\n",
    "        for different models.\n",
    "\n",
    "    Returns:\n",
    "      A tuple of (model output, new mutable states)\n",
    "    \"\"\"\n",
    "    if not train:\n",
    "      model.eval()\n",
    "      return model(x, labels)\n",
    "    else:\n",
    "      model.train()\n",
    "      return model(x, labels)\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "def get_optimizer(config, params):\n",
    "  \"\"\"Returns a flax optimizer object based on `config`.\"\"\"\n",
    "  if config.optim.optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(params, lr=config.optim.lr, betas=(config.optim.beta1, 0.999), eps=config.optim.eps,\n",
    "                           weight_decay=config.optim.weight_decay)\n",
    "  else:\n",
    "    raise NotImplementedError(\n",
    "      f'Optimizer {config.optim.optimizer} not supported yet!')\n",
    "\n",
    "  return optimizer\n",
    "\n",
    "\n",
    "def optimization_manager(config):\n",
    "  \"\"\"Returns an optimize_fn based on `config`.\"\"\"\n",
    "\n",
    "  def optimize_fn(optimizer, params, step, lr=config.optim.lr,\n",
    "                  warmup=config.optim.warmup,\n",
    "                  grad_clip=config.optim.grad_clip):\n",
    "    \"\"\"Optimizes with warmup and gradient clipping (disabled if negative).\"\"\"\n",
    "    if warmup > 0:\n",
    "      for g in optimizer.param_groups:\n",
    "        g['lr'] = lr * np.minimum(step / warmup, 1.0)\n",
    "    if grad_clip >= 0:\n",
    "      torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "  return optimize_fn\n",
    "\n",
    "\n",
    "def get_sde_loss_fn(sde, train, reduce_mean=True, continuous=True, likelihood_weighting=True, eps=1e-5):\n",
    "  \"\"\"Create a loss function for training with arbirary SDEs.\n",
    "\n",
    "  Args:\n",
    "    sde: An `sde_lib.SDE` object that represents the forward SDE.\n",
    "    train: `True` for training loss and `False` for evaluation loss.\n",
    "    reduce_mean: If `True`, average the loss across data dimensions. Otherwise sum the loss across data dimensions.\n",
    "    continuous: `True` indicates that the model is defined to take continuous time steps. Otherwise it requires\n",
    "      ad-hoc interpolation to take continuous time steps.\n",
    "    likelihood_weighting: If `True`, weight the mixture of score matching losses\n",
    "      according to https://arxiv.org/abs/2101.09258; otherwise use the weighting recommended in our paper.\n",
    "    eps: A `float` number. The smallest time step to sample from.\n",
    "\n",
    "  Returns:\n",
    "    A loss function.\n",
    "  \"\"\"\n",
    "  reduce_op = torch.mean if reduce_mean else lambda *args, **kwargs: 0.5 * torch.sum(*args, **kwargs)\n",
    "\n",
    "  def loss_fn(model, batch):\n",
    "    \"\"\"Compute the loss function.\n",
    "\n",
    "    Args:\n",
    "      model: A score model.\n",
    "      batch: A mini-batch of training data.\n",
    "\n",
    "    Returns:\n",
    "      loss: A scalar that represents the average loss value across the mini-batch.\n",
    "    \"\"\"\n",
    "    score_fn = get_score_fn(sde, model, train=train, continuous=continuous)\n",
    "    t = torch.rand(batch.shape[0], device=batch.device) * (sde.T - eps) + eps\n",
    "    z = torch.randn_like(batch)\n",
    "    mean, std = sde.marginal_prob(batch, t)\n",
    "    perturbed_data = mean + std[:, None, None, None] * z\n",
    "    score = score_fn(perturbed_data, t)\n",
    "\n",
    "    if not likelihood_weighting:\n",
    "      losses = torch.square(score * std[:, None, None, None] + z)\n",
    "      losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1)\n",
    "    else:\n",
    "      g2 = sde.sde(torch.zeros_like(batch), t)[1] ** 2\n",
    "      losses = torch.square(score + z / std[:, None, None, None])\n",
    "      losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1) * g2\n",
    "\n",
    "    loss = torch.mean(losses)\n",
    "    return loss\n",
    "\n",
    "  return loss_fn\n",
    "\n",
    "\n",
    "\n",
    "def get_ddpm_loss_fn(vpsde, train, reduce_mean=True):\n",
    "  \"\"\"Legacy code to reproduce previous results on DDPM. Not recommended for new work.\"\"\"\n",
    "  assert isinstance(vpsde, VPSDE), \"DDPM training only works for VPSDEs.\"\n",
    "\n",
    "  reduce_op = torch.mean if reduce_mean else lambda *args, **kwargs: 0.5 * torch.sum(*args, **kwargs)\n",
    "\n",
    "  def loss_fn(model, batch):\n",
    "    model_fn = get_model_fn(model, train=train)\n",
    "    labels = torch.randint(0, vpsde.N, (batch.shape[0],), device=batch.device)\n",
    "    sqrt_alphas_cumprod = vpsde.sqrt_alphas_cumprod.to(batch.device)\n",
    "    sqrt_1m_alphas_cumprod = vpsde.sqrt_1m_alphas_cumprod.to(batch.device)\n",
    "    noise = torch.randn_like(batch)\n",
    "    perturbed_data = sqrt_alphas_cumprod[labels, None, None, None] * batch + \\\n",
    "                     sqrt_1m_alphas_cumprod[labels, None, None, None] * noise\n",
    "    score = model_fn(perturbed_data, labels)\n",
    "    losses = torch.square(score - noise)\n",
    "    losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1)\n",
    "    loss = torch.mean(losses)\n",
    "    return loss\n",
    "\n",
    "  return loss_fn\n",
    "\n",
    "\n",
    "def get_step_fn(sde, train, optimize_fn=None, reduce_mean=False, continuous=True, likelihood_weighting=False):\n",
    "  \"\"\"Create a one-step training/evaluation function.\n",
    "\n",
    "  Args:\n",
    "    sde: An `sde_lib.SDE` object that represents the forward SDE.\n",
    "    optimize_fn: An optimization function.\n",
    "    reduce_mean: If `True`, average the loss across data dimensions. Otherwise sum the loss across data dimensions.\n",
    "    continuous: `True` indicates that the model is defined to take continuous time steps.\n",
    "    likelihood_weighting: If `True`, weight the mixture of score matching losses according to\n",
    "      https://arxiv.org/abs/2101.09258; otherwise use the weighting recommended by our paper.\n",
    "\n",
    "  Returns:\n",
    "    A one-step function for training or evaluation.\n",
    "  \"\"\"\n",
    "  if continuous:\n",
    "    loss_fn = get_sde_loss_fn(sde, train, reduce_mean=reduce_mean,\n",
    "                              continuous=True, likelihood_weighting=likelihood_weighting)\n",
    "  else:\n",
    "    assert not likelihood_weighting, \"Likelihood weighting is not supported for original SMLD/DDPM training.\"\n",
    "    if isinstance(sde, VPSDE):\n",
    "      loss_fn = get_ddpm_loss_fn(sde, train, reduce_mean=reduce_mean)\n",
    "    else:\n",
    "      raise ValueError(f\"Discrete training for {sde.__class__.__name__} is not recommended.\")\n",
    "\n",
    "  def step_fn(state, batch):\n",
    "    \"\"\"Running one step of training or evaluation.\n",
    "\n",
    "    This function will undergo `jax.lax.scan` so that multiple steps can be pmapped and jit-compiled together\n",
    "    for faster execution.\n",
    "\n",
    "    Args:\n",
    "      state: A dictionary of training information, containing the score model, optimizer,\n",
    "       EMA status, and number of optimization steps.\n",
    "      batch: A mini-batch of training/evaluation data.\n",
    "\n",
    "    Returns:\n",
    "      loss: The average loss value of this state.\n",
    "    \"\"\"\n",
    "    model = state['model']\n",
    "    if train:\n",
    "      optimizer = state['optimizer']\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_fn(model, batch)\n",
    "      loss.backward()\n",
    "      optimize_fn(optimizer, model.parameters(), step=state['step'])\n",
    "      state['step'] += 1\n",
    "      #state['ema'].update(model.parameters())\n",
    "    else:\n",
    "      with torch.no_grad():\n",
    "        ema = state['ema']\n",
    "        ema.store(model.parameters())\n",
    "        ema.copy_to(model.parameters())\n",
    "        loss = loss_fn(model, batch)\n",
    "        ema.restore(model.parameters())\n",
    "\n",
    "    return loss\n",
    "\n",
    "  return step_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31ae4f-068e-4fc3-a2b1-bbd30cf36f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import abc\n",
    "\n",
    "from scipy import integrate\n",
    "def to_flattened_numpy(x):\n",
    "    \n",
    "  \"\"\"Flatten a torch tensor `x` and convert it to numpy.\"\"\"\n",
    "  return x.detach().cpu().numpy().reshape((-1,))\n",
    "\n",
    "\n",
    "def from_flattened_numpy(x, shape):\n",
    "  \"\"\"Form a torch tensor with the given `shape` from a flattened numpy array `x`.\"\"\"\n",
    "  return torch.from_numpy(x.reshape(shape))\n",
    "\n",
    "\n",
    "def get_sampling_fn(config, sde, shape, inverse_scaler, eps):\n",
    "  \"\"\"Create a sampling function.\n",
    "\n",
    "  Args:\n",
    "    config: A `ml_collections.ConfigDict` object that contains all configuration information.\n",
    "    sde: A `sde_lib.SDE` object that represents the forward SDE.\n",
    "    shape: A sequence of integers representing the expected shape of a single sample.\n",
    "    inverse_scaler: The inverse data normalizer function.\n",
    "    eps: A `float` number. The reverse-time SDE is only integrated to `eps` for numerical stability.\n",
    "\n",
    "  Returns:\n",
    "    A function that takes random states and a replicated training state and outputs samples with the\n",
    "      trailing dimensions matching `shape`.\n",
    "  \"\"\"\n",
    "\n",
    "  sampler_name = config.sampling.method\n",
    "  # Probability flow ODE sampling with black-box ODE solvers\n",
    "  if sampler_name.lower() == 'ode':\n",
    "    sampling_fn = get_ode_sampler(sde=sde,\n",
    "                                  shape=shape,\n",
    "                                  inverse_scaler=inverse_scaler,\n",
    "                                  denoise=config.sampling.noise_removal,\n",
    "                                  eps=eps,\n",
    "                                  device=config.device)\n",
    "  # Predictor-Corrector sampling. Predictor-only and Corrector-only samplers are special cases.\n",
    "  elif sampler_name.lower() == 'pc':\n",
    "    predictor = EulerMaruyamaPredictor\n",
    "    corrector = NoneCorrector\n",
    "    sampling_fn = get_pc_sampler(sde=sde,\n",
    "                                 shape=shape,\n",
    "                                 predictor=predictor,\n",
    "                                 corrector=corrector,\n",
    "                                 inverse_scaler=inverse_scaler,\n",
    "                                 snr=config.sampling.snr,\n",
    "                                 n_steps=config.sampling.n_steps_each,\n",
    "                                 probability_flow=config.sampling.probability_flow,\n",
    "                                 continuous=config.training.continuous,\n",
    "                                 denoise=config.sampling.noise_removal,\n",
    "                                 eps=eps,\n",
    "                                 device=config.device)\n",
    "  else:\n",
    "    raise ValueError(f\"Sampler name {sampler_name} unknown.\")\n",
    "\n",
    "  return sampling_fn\n",
    "\n",
    "\n",
    "class Predictor(abc.ABC):\n",
    "  \"\"\"The abstract class for a predictor algorithm.\"\"\"\n",
    "\n",
    "  def __init__(self, sde, score_fn, probability_flow=False):\n",
    "    super().__init__()\n",
    "    self.sde = sde\n",
    "    # Compute the reverse SDE/ODE\n",
    "    self.rsde = sde.reverse(score_fn, probability_flow)\n",
    "    self.score_fn = score_fn\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def update_fn(self, x, t):\n",
    "    \"\"\"One update of the predictor.\n",
    "\n",
    "    Args:\n",
    "      x: A PyTorch tensor representing the current state\n",
    "      t: A Pytorch tensor representing the current time step.\n",
    "\n",
    "    Returns:\n",
    "      x: A PyTorch tensor of the next state.\n",
    "      x_mean: A PyTorch tensor. The next state without random noise. Useful for denoising.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class Corrector(abc.ABC):\n",
    "  \"\"\"The abstract class for a corrector algorithm.\"\"\"\n",
    "\n",
    "  def __init__(self, sde, score_fn, snr, n_steps):\n",
    "    super().__init__()\n",
    "    self.sde = sde\n",
    "    self.score_fn = score_fn\n",
    "    self.snr = snr\n",
    "    self.n_steps = n_steps\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def update_fn(self, x, t):\n",
    "    \"\"\"One update of the corrector.\n",
    "\n",
    "    Args:\n",
    "      x: A PyTorch tensor representing the current state\n",
    "      t: A PyTorch tensor representing the current time step.\n",
    "\n",
    "    Returns:\n",
    "      x: A PyTorch tensor of the next state.\n",
    "      x_mean: A PyTorch tensor. The next state without random noise. Useful for denoising.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class EulerMaruyamaPredictor(Predictor):\n",
    "  def __init__(self, sde, score_fn, probability_flow=False):\n",
    "    super().__init__(sde, score_fn, probability_flow)\n",
    "\n",
    "  def update_fn(self, x, t):\n",
    "    dt = -1. / self.rsde.N\n",
    "    z = torch.randn_like(x)\n",
    "    drift, diffusion = self.rsde.sde(x, t)\n",
    "    x_mean = x + drift * dt\n",
    "    x = x_mean + diffusion[:, None, None, None] * np.sqrt(-dt) * z\n",
    "    return x, x_mean\n",
    "\n",
    "\n",
    "class ReverseDiffusionPredictor(Predictor):\n",
    "  def __init__(self, sde, score_fn, probability_flow=False):\n",
    "    super().__init__(sde, score_fn, probability_flow)\n",
    "\n",
    "  def update_fn(self, x, t):\n",
    "    f, G = self.rsde.discretize(x, t)\n",
    "    z = torch.randn_like(x)\n",
    "    x_mean = x - f\n",
    "    x = x_mean + G[:, None, None, None] * z\n",
    "    return x, x_mean\n",
    "\n",
    "class AncestralSamplingPredictor(Predictor):\n",
    "  \"\"\"The ancestral sampling predictor. Currently only supports VE/VP SDEs.\"\"\"\n",
    "\n",
    "  def __init__(self, sde, score_fn, probability_flow=False):\n",
    "    super().__init__(sde, score_fn, probability_flow)\n",
    "    if not isinstance(sde, sde_lib.VPSDE) and not isinstance(sde, sde_lib.VESDE):\n",
    "      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n",
    "    assert not probability_flow, \"Probability flow not supported by ancestral sampling\"\n",
    "\n",
    "  def vesde_update_fn(self, x, t):\n",
    "    sde = self.sde\n",
    "    timestep = (t * (sde.N - 1) / sde.T).long()\n",
    "    sigma = sde.discrete_sigmas[timestep]\n",
    "    adjacent_sigma = torch.where(timestep == 0, torch.zeros_like(t), sde.discrete_sigmas.to(t.device)[timestep - 1])\n",
    "    score = self.score_fn(x, t)\n",
    "    x_mean = x + score * (sigma ** 2 - adjacent_sigma ** 2)[:, None, None, None]\n",
    "    std = torch.sqrt((adjacent_sigma ** 2 * (sigma ** 2 - adjacent_sigma ** 2)) / (sigma ** 2))\n",
    "    noise = torch.randn_like(x)\n",
    "    x = x_mean + std[:, None, None, None] * noise\n",
    "    return x, x_mean\n",
    "\n",
    "  def vpsde_update_fn(self, x, t):\n",
    "    sde = self.sde\n",
    "    timestep = (t * (sde.N - 1) / sde.T).long()\n",
    "    beta = sde.discrete_betas.to(t.device)[timestep]\n",
    "    score = self.score_fn(x, t)\n",
    "    x_mean = (x + beta[:, None, None, None] * score) / torch.sqrt(1. - beta)[:, None, None, None]\n",
    "    noise = torch.randn_like(x)\n",
    "    x = x_mean + torch.sqrt(beta)[:, None, None, None] * noise\n",
    "    return x, x_mean\n",
    "\n",
    "  def update_fn(self, x, t):\n",
    "    if isinstance(self.sde, sde_lib.VESDE):\n",
    "      return self.vesde_update_fn(x, t)\n",
    "    elif isinstance(self.sde, sde_lib.VPSDE):\n",
    "      return self.vpsde_update_fn(x, t)\n",
    "\n",
    "\n",
    "class NonePredictor(Predictor):\n",
    "  \"\"\"An empty predictor that does nothing.\"\"\"\n",
    "\n",
    "  def __init__(self, sde, score_fn, probability_flow=False):\n",
    "    pass\n",
    "\n",
    "  def update_fn(self, x, t):\n",
    "    return x, x\n",
    "\n",
    "\n",
    "class LangevinCorrector(Corrector):\n",
    "  def __init__(self, sde, score_fn, snr, n_steps):\n",
    "    super().__init__(sde, score_fn, snr, n_steps)\n",
    "    if not isinstance(sde, sde_lib.VPSDE) \\\n",
    "        and not isinstance(sde, sde_lib.VESDE) \\\n",
    "        and not isinstance(sde, sde_lib.subVPSDE):\n",
    "      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n",
    "\n",
    "  def update_fn(self, x, t):\n",
    "    sde = self.sde\n",
    "    score_fn = self.score_fn\n",
    "    n_steps = self.n_steps\n",
    "    target_snr = self.snr\n",
    "    if isinstance(sde, sde_lib.VPSDE) or isinstance(sde, sde_lib.subVPSDE):\n",
    "      timestep = (t * (sde.N - 1) / sde.T).long()\n",
    "      alpha = sde.alphas.to(t.device)[timestep]\n",
    "    else:\n",
    "      alpha = torch.ones_like(t)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "      grad = score_fn(x, t)\n",
    "      noise = torch.randn_like(x)\n",
    "      grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n",
    "      noise_norm = torch.norm(noise.reshape(noise.shape[0], -1), dim=-1).mean()\n",
    "      step_size = (target_snr * noise_norm / grad_norm) ** 2 * 2 * alpha\n",
    "      x_mean = x + step_size[:, None, None, None] * grad\n",
    "      x = x_mean + torch.sqrt(step_size * 2)[:, None, None, None] * noise\n",
    "\n",
    "    return x, x_mean\n",
    "\n",
    "\n",
    "class AnnealedLangevinDynamics(Corrector):\n",
    "  \"\"\"The original annealed Langevin dynamics predictor in NCSN/NCSNv2.\n",
    "\n",
    "  We include this corrector only for completeness. It was not directly used in our paper.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, sde, score_fn, snr, n_steps):\n",
    "    super().__init__(sde, score_fn, snr, n_steps)\n",
    "    if not isinstance(sde, sde_lib.VPSDE) \\\n",
    "        and not isinstance(sde, sde_lib.VESDE) \\\n",
    "        and not isinstance(sde, sde_lib.subVPSDE):\n",
    "      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n",
    "\n",
    "  def update_fn(self, x, t):\n",
    "    sde = self.sde\n",
    "    score_fn = self.score_fn\n",
    "    n_steps = self.n_steps\n",
    "    target_snr = self.snr\n",
    "    if isinstance(sde, sde_lib.VPSDE) or isinstance(sde, sde_lib.subVPSDE):\n",
    "      timestep = (t * (sde.N - 1) / sde.T).long()\n",
    "      alpha = sde.alphas.to(t.device)[timestep]\n",
    "    else:\n",
    "      alpha = torch.ones_like(t)\n",
    "\n",
    "    std = self.sde.marginal_prob(x, t)[1]\n",
    "\n",
    "    for i in range(n_steps):\n",
    "      grad = score_fn(x, t)\n",
    "      noise = torch.randn_like(x)\n",
    "      step_size = (target_snr * std) ** 2 * 2 * alpha\n",
    "      x_mean = x + step_size[:, None, None, None] * grad\n",
    "      x = x_mean + noise * torch.sqrt(step_size * 2)[:, None, None, None]\n",
    "\n",
    "    return x, x_mean\n",
    "\n",
    "\n",
    "class NoneCorrector(Corrector):\n",
    "  \"\"\"An empty corrector that does nothing.\"\"\"\n",
    "\n",
    "  def __init__(self, sde, score_fn, snr, n_steps):\n",
    "    pass\n",
    "\n",
    "  def update_fn(self, x, t):\n",
    "    return x, x\n",
    "\n",
    "\n",
    "def shared_predictor_update_fn(x, t, sde, model, predictor, probability_flow, continuous):\n",
    "  \"\"\"A wrapper that configures and returns the update function of predictors.\"\"\"\n",
    "  score_fn = get_score_fn(sde, model, train=False, continuous=continuous)\n",
    "  if predictor is None:\n",
    "    # Corrector-only sampler\n",
    "    predictor_obj = NonePredictor(sde, score_fn, probability_flow)\n",
    "  else:\n",
    "    predictor_obj = predictor(sde, score_fn, probability_flow)\n",
    "  return predictor_obj.update_fn(x, t)\n",
    "\n",
    "\n",
    "def shared_corrector_update_fn(x, t, sde, model, corrector, continuous, snr, n_steps):\n",
    "  \"\"\"A wrapper tha configures and returns the update function of correctors.\"\"\"\n",
    "  score_fn = get_score_fn(sde, model, train=False, continuous=continuous)\n",
    "  if corrector is None:\n",
    "    # Predictor-only sampler\n",
    "    corrector_obj = NoneCorrector(sde, score_fn, snr, n_steps)\n",
    "  else:\n",
    "    corrector_obj = corrector(sde, score_fn, snr, n_steps)\n",
    "  return corrector_obj.update_fn(x, t)\n",
    "\n",
    "\n",
    "def get_pc_sampler(sde, shape, predictor, corrector, inverse_scaler, snr,\n",
    "                   n_steps=1, probability_flow=False, continuous=False,\n",
    "                   denoise=True, eps=1e-3, device='cuda'):\n",
    "  \"\"\"Create a Predictor-Corrector (PC) sampler.\n",
    "\n",
    "  Args:\n",
    "    sde: An `sde_lib.SDE` object representing the forward SDE.\n",
    "    shape: A sequence of integers. The expected shape of a single sample.\n",
    "    predictor: A subclass of `sampling.Predictor` representing the predictor algorithm.\n",
    "    corrector: A subclass of `sampling.Corrector` representing the corrector algorithm.\n",
    "    inverse_scaler: The inverse data normalizer.\n",
    "    snr: A `float` number. The signal-to-noise ratio for configuring correctors.\n",
    "    n_steps: An integer. The number of corrector steps per predictor update.\n",
    "    probability_flow: If `True`, solve the reverse-time probability flow ODE when running the predictor.\n",
    "    continuous: `True` indicates that the score model was continuously trained.\n",
    "    denoise: If `True`, add one-step denoising to the final samples.\n",
    "    eps: A `float` number. The reverse-time SDE and ODE are integrated to `epsilon` to avoid numerical issues.\n",
    "    device: PyTorch device.\n",
    "\n",
    "  Returns:\n",
    "    A sampling function that returns samples and the number of function evaluations during sampling.\n",
    "  \"\"\"\n",
    "  # Create predictor & corrector update functions\n",
    "  predictor_update_fn = functools.partial(shared_predictor_update_fn,\n",
    "                                          sde=sde,\n",
    "                                          predictor=predictor,\n",
    "                                          probability_flow=probability_flow,\n",
    "                                          continuous=continuous)\n",
    "  corrector_update_fn = functools.partial(shared_corrector_update_fn,\n",
    "                                          sde=sde,\n",
    "                                          corrector=corrector,\n",
    "                                          continuous=continuous,\n",
    "                                          snr=snr,\n",
    "                                          n_steps=n_steps)\n",
    "\n",
    "  def pc_sampler(model):\n",
    "    \"\"\" The PC sampler funciton.\n",
    "\n",
    "    Args:\n",
    "      model: A score model.\n",
    "    Returns:\n",
    "      Samples, number of function evaluations.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "      # Initial sample\n",
    "      x = sde.prior_sampling(shape).to(device)\n",
    "      timesteps = torch.linspace(sde.T, eps, sde.N, device=device)\n",
    "\n",
    "      for i in range(sde.N):\n",
    "        t = timesteps[i]\n",
    "        vec_t = torch.ones(shape[0], device=t.device) * t\n",
    "        x, x_mean = corrector_update_fn(x, vec_t, model=model)\n",
    "        x, x_mean = predictor_update_fn(x, vec_t, model=model)\n",
    "\n",
    "      return inverse_scaler(x_mean if denoise else x), sde.N * (n_steps + 1)\n",
    "\n",
    "  return pc_sampler\n",
    "\n",
    "\n",
    "def get_ode_sampler(sde, shape, inverse_scaler,\n",
    "                    denoise=False, rtol=1e-5, atol=1e-5,\n",
    "                    method='RK45', eps=1e-3, device='cuda'):\n",
    "  \"\"\"Probability flow ODE sampler with the black-box ODE solver.\n",
    "\n",
    "  Args:\n",
    "    sde: An `sde_lib.SDE` object that represents the forward SDE.\n",
    "    shape: A sequence of integers. The expected shape of a single sample.\n",
    "    inverse_scaler: The inverse data normalizer.\n",
    "    denoise: If `True`, add one-step denoising to final samples.\n",
    "    rtol: A `float` number. The relative tolerance level of the ODE solver.\n",
    "    atol: A `float` number. The absolute tolerance level of the ODE solver.\n",
    "    method: A `str`. The algorithm used for the black-box ODE solver.\n",
    "      See the documentation of `scipy.integrate.solve_ivp`.\n",
    "    eps: A `float` number. The reverse-time SDE/ODE will be integrated to `eps` for numerical stability.\n",
    "    device: PyTorch device.\n",
    "\n",
    "  Returns:\n",
    "    A sampling function that returns samples and the number of function evaluations during sampling.\n",
    "  \"\"\"\n",
    "\n",
    "  def denoise_update_fn(model, x):\n",
    "    score_fn = get_score_fn(sde, model, train=False, continuous=True)\n",
    "    # Reverse diffusion predictor for denoising\n",
    "    predictor_obj = ReverseDiffusionPredictor(sde, score_fn, probability_flow=False)\n",
    "    vec_eps = torch.ones(x.shape[0], device=x.device) * eps\n",
    "    _, x = predictor_obj.update_fn(x, vec_eps)\n",
    "    return x\n",
    "\n",
    "  def drift_fn(model, x, t):\n",
    "    \"\"\"Get the drift function of the reverse-time SDE.\"\"\"\n",
    "    score_fn = get_score_fn(sde, model, train=False, continuous=True)\n",
    "    rsde = sde.reverse(score_fn, probability_flow=True)\n",
    "    return rsde.sde(x, t)[0]\n",
    "\n",
    "  def ode_sampler(model, z=None):\n",
    "    \"\"\"The probability flow ODE sampler with black-box ODE solver.\n",
    "\n",
    "    Args:\n",
    "      model: A score model.\n",
    "      z: If present, generate samples from latent code `z`.\n",
    "    Returns:\n",
    "      samples, number of function evaluations.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "      # Initial sample\n",
    "      if z is None:\n",
    "        # If not represent, sample the latent code from the prior distibution of the SDE.\n",
    "        x = sde.prior_sampling(shape).to(device)\n",
    "      else:\n",
    "        x = z\n",
    "\n",
    "      def ode_func(t, x):\n",
    "        x = from_flattened_numpy(x, shape).to(device).type(torch.float32)\n",
    "        vec_t = torch.ones(shape[0], device=x.device) * t\n",
    "        drift = drift_fn(model, x, vec_t)\n",
    "        return to_flattened_numpy(drift)\n",
    "\n",
    "      # Black-box ODE solver for the probability flow ODE\n",
    "      solution = integrate.solve_ivp(ode_func, (sde.T, eps), to_flattened_numpy(x),\n",
    "                                     rtol=rtol, atol=atol, method=method)\n",
    "      nfe = solution.nfev\n",
    "      x = torch.tensor(solution.y[:, -1]).reshape(shape).to(device).type(torch.float32)\n",
    "\n",
    "      # Denoising is equivalent to running one predictor step without adding noise\n",
    "      if denoise:\n",
    "        x = denoise_update_fn(model, x)\n",
    "\n",
    "      x = inverse_scaler(x)\n",
    "      return x, nfe\n",
    "\n",
    "  return ode_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be1a59-9d6b-40fe-bd57-12615bf75e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_scaler(config):\n",
    "  \"\"\"Data normalizer. Assume data are always in [0, 1].\"\"\"\n",
    "  if config.data.centered:\n",
    "    # Rescale to [-1, 1]\n",
    "    return lambda x: x * 2. - 1.\n",
    "  else:\n",
    "    return lambda x: x\n",
    "\n",
    "\n",
    "def get_data_inverse_scaler(config):\n",
    "  \"\"\"Inverse data normalizer.\"\"\"\n",
    "  if config.data.centered:\n",
    "    # Rescale [-1, 1] to [0, 1]\n",
    "    return lambda x: (x + 1.) / 2.\n",
    "  else:\n",
    "    return lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3719e-25cd-4de9-b36d-34a99ec5a8f4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b166a28-d2fb-49e8-af3f-ba140ba86249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARQUET_DATABASE_URL=\"https://huggingface.co/datasets/0xJustin/Dungeons-and-Diffusion/resolve/main/data/train-00000-of-00001-9b40395dcd3257f2.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38577e20-b3f2-4fc4-b859-473e27b6d7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize model.\n",
    "config=get_default_configs()\n",
    "score_model = DDPM(config)\n",
    "score_model.load_state_dict(torch.load('ckpt.pth',map_location=config.device))\n",
    "score_model = score_model.to(config.device)\n",
    "#ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n",
    "optimizer = get_optimizer(config, score_model.parameters())\n",
    "#state = dict(optimizer=optimizer, model=score_model, ema=ema, step=0)\n",
    "state = dict(optimizer=optimizer, model=score_model, step=0)\n",
    "initial_step = int(state['step'])\n",
    "\n",
    "\n",
    "# Build data iterators\n",
    "dataset = HeroicDataset(PARQUET_DATABASE_URL)\n",
    "data_loader = DataLoader(dataset, batch_size=config.training.batch_size, shuffle=True, num_workers=1,pin_memory=True)\n",
    "\n",
    "\n",
    "# Create data normalizer and its inverse\n",
    "scaler = get_data_scaler(config)\n",
    "inverse_scaler = get_data_inverse_scaler(config)\n",
    "\n",
    "\n",
    "# Setup SDEs\n",
    "sde = VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n",
    "sampling_eps = 1e-3\n",
    "\n",
    "# Build one-step training and evaluation functions\n",
    "optimize_fn = optimization_manager(config)\n",
    "continuous = config.training.continuous\n",
    "reduce_mean = config.training.reduce_mean\n",
    "likelihood_weighting = config.training.likelihood_weighting\n",
    "train_step_fn = get_step_fn(sde, train=True, optimize_fn=optimize_fn,\n",
    "                                 reduce_mean=reduce_mean, continuous=continuous,\n",
    "                                 likelihood_weighting=likelihood_weighting)\n",
    "\n",
    "\n",
    "num_train_steps = config.training.n_iters\n",
    "\n",
    "if config.training.snapshot_sampling:\n",
    "    sampling_shape = (config.training.batch_size, config.data.num_channels,\n",
    "                      config.data.image_size, config.data.image_size)\n",
    "    sampling_fn = get_sampling_fn(config, sde, sampling_shape, inverse_scaler, sampling_eps)\n",
    "# In case there are multiple hosts (e.g., TPU pods), only log to host 0\n",
    "print(\"Starting training loop at step %d.\" % (initial_step,))\n",
    "\n",
    "for step in range(initial_step, num_train_steps + 1):\n",
    "    for _, y in data_loader:\n",
    "        y = y.to(config.device)    \n",
    "        loss = train_step_fn(state, y)\n",
    "        if step % config.training.log_freq == 0:\n",
    "          print(\"step: %d, training_loss: %.5e\" % (step, loss.item()))\n",
    "          torch.save(score_model.state_dict(), 'ckpt.pth')\n",
    "          sample, n = sampling_fn(score_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466a1b0-c30b-4021-9ebf-319f41f65099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
