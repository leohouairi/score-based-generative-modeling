{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150f5e0-8e61-4a4a-bc09-bb7fde5cc374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fbace-e1e2-4cba-8cfd-4f27041d02ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ml-collections==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c62e4-f717-4ce4-ac9a-598999b833a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ecbef4-2053-4dab-ac38-35d2e3c94f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from heroic_gm.models.heroic_nn import HeroicScoreNet\n",
    "from heroic_gm.data.heroic_dataset import HeroicDataset\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf8052-8f6b-44a4-b803-c0889016778b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SDE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2bba9-caf6-42f7-b5e5-6f475435ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yang Song sde_lib class\n",
    "\"\"\"Abstract SDE classes, Reverse SDE, and VE/VP SDEs.\"\"\"\n",
    "import abc\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SDE(abc.ABC):\n",
    "  \"\"\"SDE abstract class. Functions are designed for a mini-batch of inputs.\"\"\"\n",
    "\n",
    "  def __init__(self, N):\n",
    "    \"\"\"Construct an SDE.\n",
    "\n",
    "    Args:\n",
    "      N: number of discretization time steps.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.N = N\n",
    "\n",
    "  @property\n",
    "  @abc.abstractmethod\n",
    "  def T(self):\n",
    "    \"\"\"End time of the SDE.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def sde(self, x, t):\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def marginal_prob(self, x, t):\n",
    "    \"\"\"Parameters to determine the marginal distribution of the SDE, $p_t(x)$.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def prior_sampling(self, shape):\n",
    "    \"\"\"Generate one sample from the prior distribution, $p_T(x)$.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def prior_logp(self, z):\n",
    "    \"\"\"Compute log-density of the prior distribution.\n",
    "\n",
    "    Useful for computing the log-likelihood via probability flow ODE.\n",
    "\n",
    "    Args:\n",
    "      z: latent code\n",
    "    Returns:\n",
    "      log probability density\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def discretize(self, x, t):\n",
    "    \"\"\"Discretize the SDE in the form: x_{i+1} = x_i + f_i(x_i) + G_i z_i.\n",
    "\n",
    "    Useful for reverse diffusion sampling and probabiliy flow sampling.\n",
    "    Defaults to Euler-Maruyama discretization.\n",
    "\n",
    "    Args:\n",
    "      x: a torch tensor\n",
    "      t: a torch float representing the time step (from 0 to `self.T`)\n",
    "\n",
    "    Returns:\n",
    "      f, G\n",
    "    \"\"\"\n",
    "    dt = 1 / self.N\n",
    "    drift, diffusion = self.sde(x, t)\n",
    "    f = drift * dt\n",
    "    G = diffusion * torch.sqrt(torch.tensor(dt, device=t.device))\n",
    "    return f, G\n",
    "\n",
    "  def reverse(self, score_fn, probability_flow=False):\n",
    "    \"\"\"Create the reverse-time SDE/ODE.\n",
    "\n",
    "    Args:\n",
    "      score_fn: A time-dependent score-based model that takes x and t and returns the score.\n",
    "      probability_flow: If `True`, create the reverse-time ODE used for probability flow sampling.\n",
    "    \"\"\"\n",
    "    N = self.N\n",
    "    T = self.T\n",
    "    sde_fn = self.sde\n",
    "    discretize_fn = self.discretize\n",
    "\n",
    "    # Build the class for reverse-time SDE.\n",
    "    class RSDE(self.__class__):\n",
    "      def __init__(self):\n",
    "        self.N = N\n",
    "        self.probability_flow = probability_flow\n",
    "\n",
    "      @property\n",
    "      def T(self):\n",
    "        return T\n",
    "\n",
    "      def sde(self, x, t):\n",
    "        \"\"\"Create the drift and diffusion functions for the reverse SDE/ODE.\"\"\"\n",
    "        drift, diffusion = sde_fn(x, t)\n",
    "        score = score_fn(x, t)\n",
    "        drift = drift - diffusion[:, None, None, None] ** 2 * score * (0.5 if self.probability_flow else 1.)\n",
    "        # Set the diffusion function to zero for ODEs.\n",
    "        diffusion = 0. if self.probability_flow else diffusion\n",
    "        return drift, diffusion\n",
    "\n",
    "      def discretize(self, x, t):\n",
    "        \"\"\"Create discretized iteration rules for the reverse diffusion sampler.\"\"\"\n",
    "        f, G = discretize_fn(x, t)\n",
    "        rev_f = f - G[:, None, None, None] ** 2 * score_fn(x, t) * (0.5 if self.probability_flow else 1.)\n",
    "        rev_G = torch.zeros_like(G) if self.probability_flow else G\n",
    "        return rev_f, rev_G\n",
    "\n",
    "    return RSDE()\n",
    "\n",
    "\n",
    "class VPSDE(SDE):\n",
    "  def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n",
    "    \"\"\"Construct a Variance Preserving SDE.\n",
    "\n",
    "    Args:\n",
    "      beta_min: value of beta(0)\n",
    "      beta_max: value of beta(1)\n",
    "      N: number of discretization steps\n",
    "    \"\"\"\n",
    "    super().__init__(N)\n",
    "    self.beta_0 = beta_min\n",
    "    self.beta_1 = beta_max\n",
    "    self.N = N\n",
    "    self.discrete_betas = torch.linspace(beta_min / N, beta_max / N, N)\n",
    "    self.alphas = 1. - self.discrete_betas\n",
    "    self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "    self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "    self.sqrt_1m_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "\n",
    "  @property\n",
    "  def T(self):\n",
    "    return 1\n",
    "\n",
    "  def sde(self, x, t):\n",
    "    beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n",
    "    drift = -0.5 * beta_t[:, None, None, None] * x\n",
    "    diffusion = torch.sqrt(beta_t)\n",
    "    return drift, diffusion\n",
    "\n",
    "  def marginal_prob(self, x, t):\n",
    "    log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n",
    "    mean = torch.exp(log_mean_coeff[:, None, None, None]) * x\n",
    "    std = torch.sqrt(1. - torch.exp(2. * log_mean_coeff))\n",
    "    return mean, std\n",
    "\n",
    "  def prior_sampling(self, shape):\n",
    "    return torch.randn(*shape)\n",
    "\n",
    "  def prior_logp(self, z):\n",
    "    shape = z.shape\n",
    "    N = np.prod(shape[1:])\n",
    "    logps = -N / 2. * np.log(2 * np.pi) - torch.sum(z ** 2, dim=(1, 2, 3)) / 2.\n",
    "    return logps\n",
    "\n",
    "  def discretize(self, x, t):\n",
    "    \"\"\"DDPM discretization.\"\"\"\n",
    "    timestep = (t * (self.N - 1) / self.T).long()\n",
    "    beta = self.discrete_betas.to(x.device)[timestep]\n",
    "    alpha = self.alphas.to(x.device)[timestep]\n",
    "    sqrt_beta = torch.sqrt(beta)\n",
    "    f = torch.sqrt(alpha)[:, None, None, None] * x - x\n",
    "    G = sqrt_beta\n",
    "    return f, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fa52b-108a-4141-ad20-6abd8d5f2711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_fn(model, train=False):\n",
    "  \"\"\"Create a function to give the output of the score-based model.\n",
    "\n",
    "  Args:\n",
    "    model: The score model.\n",
    "    train: `True` for training and `False` for evaluation.\n",
    "\n",
    "  Returns:\n",
    "    A model function.\n",
    "  \"\"\"\n",
    "\n",
    "  def model_fn(x, labels):\n",
    "    \"\"\"Compute the output of the score-based model.\n",
    "\n",
    "    Args:\n",
    "      x: A mini-batch of input data.\n",
    "      labels: A mini-batch of conditioning variables for time steps. Should be interpreted differently\n",
    "        for different models.\n",
    "\n",
    "    Returns:\n",
    "      A tuple of (model output, new mutable states)\n",
    "    \"\"\"\n",
    "    if not train:\n",
    "      model.eval()\n",
    "      return model(x, labels)\n",
    "    else:\n",
    "      model.train()\n",
    "      return model(x, labels)\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "\n",
    "def get_score_fn(sde, model, train=False, continuous=False):\n",
    "    \"\"\"Wraps `score_fn` so that the model output corresponds to a real time-dependent score function.\n",
    "\n",
    "    Args:\n",
    "    sde: An `sde_lib.SDE` object that represents the forward SDE.\n",
    "    model: A score model.\n",
    "    train: `True` for training and `False` for evaluation.\n",
    "    continuous: If `True`, the score-based model is expected to directly take continuous time steps.\n",
    "\n",
    "    Returns:\n",
    "    A score function.\n",
    "    \"\"\"\n",
    "    model_fn = get_model_fn(model, train=train)\n",
    "\n",
    "    def score_fn(x, t):\n",
    "        # For VP-trained models, t=0 corresponds to the lowest noise level\n",
    "        labels = t * (sde.N - 1)\n",
    "        score = model_fn(x, labels)\n",
    "        std = sde.sqrt_1m_alphas_cumprod.to(labels.device)[labels.long()]\n",
    "\n",
    "        score = -score / std[:, None, None, None]\n",
    "        return score\n",
    "\n",
    "    return score_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9aa75a-1cc8-43b5-8175-39d7be9a9194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ConditionalInstanceNorm2dPlus(nn.Module):\n",
    "  def __init__(self, num_features, num_classes, bias=True):\n",
    "    super().__init__()\n",
    "    self.num_features = num_features\n",
    "    self.bias = bias\n",
    "    self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n",
    "    if bias:\n",
    "      self.embed = nn.Embedding(num_classes, num_features * 3)\n",
    "      self.embed.weight.data[:, :2 * num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
    "      self.embed.weight.data[:, 2 * num_features:].zero_()  # Initialise bias at 0\n",
    "    else:\n",
    "      self.embed = nn.Embedding(num_classes, 2 * num_features)\n",
    "      self.embed.weight.data.normal_(1, 0.02)\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    means = torch.mean(x, dim=(2, 3))\n",
    "    m = torch.mean(means, dim=-1, keepdim=True)\n",
    "    v = torch.var(means, dim=-1, keepdim=True)\n",
    "    means = (means - m) / (torch.sqrt(v + 1e-5))\n",
    "    h = self.instance_norm(x)\n",
    "\n",
    "    if self.bias:\n",
    "      gamma, alpha, beta = self.embed(y).chunk(3, dim=-1)\n",
    "      h = h + means[..., None, None] * alpha[..., None, None]\n",
    "      out = gamma.view(-1, self.num_features, 1, 1) * h + beta.view(-1, self.num_features, 1, 1)\n",
    "    else:\n",
    "      gamma, alpha = self.embed(y).chunk(2, dim=-1)\n",
    "      h = h + means[..., None, None] * alpha[..., None, None]\n",
    "      out = gamma.view(-1, self.num_features, 1, 1) * h\n",
    "    return out\n",
    "\n",
    "def get_act(config):\n",
    "  \"\"\"Get activation functions from the config file.\"\"\"\n",
    "\n",
    "  if config.model.nonlinearity.lower() == 'elu':\n",
    "    return nn.ELU()\n",
    "  elif config.model.nonlinearity.lower() == 'relu':\n",
    "    return nn.ReLU()\n",
    "  elif config.model.nonlinearity.lower() == 'lrelu':\n",
    "    return nn.LeakyReLU(negative_slope=0.2)\n",
    "  elif config.model.nonlinearity.lower() == 'swish':\n",
    "    return nn.SiLU()\n",
    "  else:\n",
    "    raise NotImplementedError('activation function does not exist!')\n",
    "\n",
    "\n",
    "def ncsn_conv1x1(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=0):\n",
    "  \"\"\"1x1 convolution. Same as NCSNv1/v2.\"\"\"\n",
    "  conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias, dilation=dilation,\n",
    "                   padding=padding)\n",
    "  init_scale = 1e-10 if init_scale == 0 else init_scale\n",
    "  conv.weight.data *= init_scale\n",
    "  conv.bias.data *= init_scale\n",
    "  return conv\n",
    "\n",
    "\n",
    "def variance_scaling(scale, mode, distribution,\n",
    "                     in_axis=1, out_axis=0,\n",
    "                     dtype=torch.float32,\n",
    "                     device='cpu'):\n",
    "  \"\"\"Ported from JAX. \"\"\"\n",
    "\n",
    "  def _compute_fans(shape, in_axis=1, out_axis=0):\n",
    "    receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n",
    "    fan_in = shape[in_axis] * receptive_field_size\n",
    "    fan_out = shape[out_axis] * receptive_field_size\n",
    "    return fan_in, fan_out\n",
    "\n",
    "  def init(shape, dtype=dtype, device=device):\n",
    "    fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n",
    "    if mode == \"fan_in\":\n",
    "      denominator = fan_in\n",
    "    elif mode == \"fan_out\":\n",
    "      denominator = fan_out\n",
    "    elif mode == \"fan_avg\":\n",
    "      denominator = (fan_in + fan_out) / 2\n",
    "    else:\n",
    "      raise ValueError(\n",
    "        \"invalid mode for variance scaling initializer: {}\".format(mode))\n",
    "    variance = scale / denominator\n",
    "    if distribution == \"normal\":\n",
    "      return torch.randn(*shape, dtype=dtype, device=device) * np.sqrt(variance)\n",
    "    elif distribution == \"uniform\":\n",
    "      return (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n",
    "    else:\n",
    "      raise ValueError(\"invalid distribution for variance scaling initializer\")\n",
    "\n",
    "  return init\n",
    "\n",
    "\n",
    "def default_init(scale=1.):\n",
    "  \"\"\"The same initialization used in DDPM.\"\"\"\n",
    "  scale = 1e-10 if scale == 0 else scale\n",
    "  return variance_scaling(scale, 'fan_avg', 'uniform')\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "  \"\"\"Linear layer with `default_init`.\"\"\"\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "\n",
    "def ddpm_conv1x1(in_planes, out_planes, stride=1, bias=True, init_scale=1., padding=0):\n",
    "  \"\"\"1x1 convolution with DDPM initialization.\"\"\"\n",
    "  conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=padding, bias=bias)\n",
    "  conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n",
    "  nn.init.zeros_(conv.bias)\n",
    "  return conv\n",
    "\n",
    "\n",
    "def ncsn_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n",
    "  \"\"\"3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\"\"\"\n",
    "  init_scale = 1e-10 if init_scale == 0 else init_scale\n",
    "  conv = nn.Conv2d(in_planes, out_planes, stride=stride, bias=bias,\n",
    "                   dilation=dilation, padding=padding, kernel_size=3)\n",
    "  conv.weight.data *= init_scale\n",
    "  conv.bias.data *= init_scale\n",
    "  return conv\n",
    "\n",
    "\n",
    "def ddpm_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n",
    "  \"\"\"3x3 convolution with DDPM initialization.\"\"\"\n",
    "  conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding,\n",
    "                   dilation=dilation, bias=bias)\n",
    "  conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n",
    "  nn.init.zeros_(conv.bias)\n",
    "  return conv\n",
    "\n",
    "  ###########################################################################\n",
    "  # Functions below are ported over from the NCSNv1/NCSNv2 codebase:\n",
    "  # https://github.com/ermongroup/ncsn\n",
    "  # https://github.com/ermongroup/ncsnv2\n",
    "  ###########################################################################\n",
    "\n",
    "\n",
    "class CRPBlock(nn.Module):\n",
    "  def __init__(self, features, n_stages, act=nn.ReLU(), maxpool=True):\n",
    "    super().__init__()\n",
    "    self.convs = nn.ModuleList()\n",
    "    for i in range(n_stages):\n",
    "      self.convs.append(ncsn_conv3x3(features, features, stride=1, bias=False))\n",
    "    self.n_stages = n_stages\n",
    "    if maxpool:\n",
    "      self.pool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n",
    "    else:\n",
    "      self.pool = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "    self.act = act\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.act(x)\n",
    "    path = x\n",
    "    for i in range(self.n_stages):\n",
    "      path = self.pool(path)\n",
    "      path = self.convs[i](path)\n",
    "      x = path + x\n",
    "    return x\n",
    "\n",
    "\n",
    "class CondCRPBlock(nn.Module):\n",
    "  def __init__(self, features, n_stages, num_classes, normalizer, act=nn.ReLU()):\n",
    "    super().__init__()\n",
    "    self.convs = nn.ModuleList()\n",
    "    self.norms = nn.ModuleList()\n",
    "    self.normalizer = normalizer\n",
    "    for i in range(n_stages):\n",
    "      self.norms.append(normalizer(features, num_classes, bias=True))\n",
    "      self.convs.append(ncsn_conv3x3(features, features, stride=1, bias=False))\n",
    "\n",
    "    self.n_stages = n_stages\n",
    "    self.pool = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n",
    "    self.act = act\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    x = self.act(x)\n",
    "    path = x\n",
    "    for i in range(self.n_stages):\n",
    "      path = self.norms[i](path, y)\n",
    "      path = self.pool(path)\n",
    "      path = self.convs[i](path)\n",
    "\n",
    "      x = path + x\n",
    "    return x\n",
    "\n",
    "\n",
    "class RCUBlock(nn.Module):\n",
    "  def __init__(self, features, n_blocks, n_stages, act=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "      for j in range(n_stages):\n",
    "        setattr(self, '{}_{}_conv'.format(i + 1, j + 1), ncsn_conv3x3(features, features, stride=1, bias=False))\n",
    "\n",
    "    self.stride = 1\n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_stages = n_stages\n",
    "    self.act = act\n",
    "\n",
    "  def forward(self, x):\n",
    "    for i in range(self.n_blocks):\n",
    "      residual = x\n",
    "      for j in range(self.n_stages):\n",
    "        x = self.act(x)\n",
    "        x = getattr(self, '{}_{}_conv'.format(i + 1, j + 1))(x)\n",
    "\n",
    "      x += residual\n",
    "    return x\n",
    "\n",
    "\n",
    "class CondRCUBlock(nn.Module):\n",
    "  def __init__(self, features, n_blocks, n_stages, num_classes, normalizer, act=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "      for j in range(n_stages):\n",
    "        setattr(self, '{}_{}_norm'.format(i + 1, j + 1), normalizer(features, num_classes, bias=True))\n",
    "        setattr(self, '{}_{}_conv'.format(i + 1, j + 1), ncsn_conv3x3(features, features, stride=1, bias=False))\n",
    "\n",
    "    self.stride = 1\n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_stages = n_stages\n",
    "    self.act = act\n",
    "    self.normalizer = normalizer\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    for i in range(self.n_blocks):\n",
    "      residual = x\n",
    "      for j in range(self.n_stages):\n",
    "        x = getattr(self, '{}_{}_norm'.format(i + 1, j + 1))(x, y)\n",
    "        x = self.act(x)\n",
    "        x = getattr(self, '{}_{}_conv'.format(i + 1, j + 1))(x)\n",
    "\n",
    "      x += residual\n",
    "    return x\n",
    "\n",
    "\n",
    "class MSFBlock(nn.Module):\n",
    "  def __init__(self, in_planes, features):\n",
    "    super().__init__()\n",
    "    assert isinstance(in_planes, list) or isinstance(in_planes, tuple)\n",
    "    self.convs = nn.ModuleList()\n",
    "    self.features = features\n",
    "\n",
    "    for i in range(len(in_planes)):\n",
    "      self.convs.append(ncsn_conv3x3(in_planes[i], features, stride=1, bias=True))\n",
    "\n",
    "  def forward(self, xs, shape):\n",
    "    sums = torch.zeros(xs[0].shape[0], self.features, *shape, device=xs[0].device)\n",
    "    for i in range(len(self.convs)):\n",
    "      h = self.convs[i](xs[i])\n",
    "      h = F.interpolate(h, size=shape, mode='bilinear', align_corners=True)\n",
    "      sums += h\n",
    "    return sums\n",
    "\n",
    "\n",
    "class CondMSFBlock(nn.Module):\n",
    "  def __init__(self, in_planes, features, num_classes, normalizer):\n",
    "    super().__init__()\n",
    "    assert isinstance(in_planes, list) or isinstance(in_planes, tuple)\n",
    "\n",
    "    self.convs = nn.ModuleList()\n",
    "    self.norms = nn.ModuleList()\n",
    "    self.features = features\n",
    "    self.normalizer = normalizer\n",
    "\n",
    "    for i in range(len(in_planes)):\n",
    "      self.convs.append(ncsn_conv3x3(in_planes[i], features, stride=1, bias=True))\n",
    "      self.norms.append(normalizer(in_planes[i], num_classes, bias=True))\n",
    "\n",
    "  def forward(self, xs, y, shape):\n",
    "    sums = torch.zeros(xs[0].shape[0], self.features, *shape, device=xs[0].device)\n",
    "    for i in range(len(self.convs)):\n",
    "      h = self.norms[i](xs[i], y)\n",
    "      h = self.convs[i](h)\n",
    "      h = F.interpolate(h, size=shape, mode='bilinear', align_corners=True)\n",
    "      sums += h\n",
    "    return sums\n",
    "\n",
    "\n",
    "class RefineBlock(nn.Module):\n",
    "  def __init__(self, in_planes, features, act=nn.ReLU(), start=False, end=False, maxpool=True):\n",
    "    super().__init__()\n",
    "\n",
    "    assert isinstance(in_planes, tuple) or isinstance(in_planes, list)\n",
    "    self.n_blocks = n_blocks = len(in_planes)\n",
    "\n",
    "    self.adapt_convs = nn.ModuleList()\n",
    "    for i in range(n_blocks):\n",
    "      self.adapt_convs.append(RCUBlock(in_planes[i], 2, 2, act))\n",
    "\n",
    "    self.output_convs = RCUBlock(features, 3 if end else 1, 2, act)\n",
    "\n",
    "    if not start:\n",
    "      self.msf = MSFBlock(in_planes, features)\n",
    "\n",
    "    self.crp = CRPBlock(features, 2, act, maxpool=maxpool)\n",
    "\n",
    "  def forward(self, xs, output_shape):\n",
    "    assert isinstance(xs, tuple) or isinstance(xs, list)\n",
    "    hs = []\n",
    "    for i in range(len(xs)):\n",
    "      h = self.adapt_convs[i](xs[i])\n",
    "      hs.append(h)\n",
    "\n",
    "    if self.n_blocks > 1:\n",
    "      h = self.msf(hs, output_shape)\n",
    "    else:\n",
    "      h = hs[0]\n",
    "\n",
    "    h = self.crp(h)\n",
    "    h = self.output_convs(h)\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "class CondRefineBlock(nn.Module):\n",
    "  def __init__(self, in_planes, features, num_classes, normalizer, act=nn.ReLU(), start=False, end=False):\n",
    "    super().__init__()\n",
    "\n",
    "    assert isinstance(in_planes, tuple) or isinstance(in_planes, list)\n",
    "    self.n_blocks = n_blocks = len(in_planes)\n",
    "\n",
    "    self.adapt_convs = nn.ModuleList()\n",
    "    for i in range(n_blocks):\n",
    "      self.adapt_convs.append(\n",
    "        CondRCUBlock(in_planes[i], 2, 2, num_classes, normalizer, act)\n",
    "      )\n",
    "\n",
    "    self.output_convs = CondRCUBlock(features, 3 if end else 1, 2, num_classes, normalizer, act)\n",
    "\n",
    "    if not start:\n",
    "      self.msf = CondMSFBlock(in_planes, features, num_classes, normalizer)\n",
    "\n",
    "    self.crp = CondCRPBlock(features, 2, num_classes, normalizer, act)\n",
    "\n",
    "  def forward(self, xs, y, output_shape):\n",
    "    assert isinstance(xs, tuple) or isinstance(xs, list)\n",
    "    hs = []\n",
    "    for i in range(len(xs)):\n",
    "      h = self.adapt_convs[i](xs[i], y)\n",
    "      hs.append(h)\n",
    "\n",
    "    if self.n_blocks > 1:\n",
    "      h = self.msf(hs, y, output_shape)\n",
    "    else:\n",
    "      h = hs[0]\n",
    "\n",
    "    h = self.crp(h, y)\n",
    "    h = self.output_convs(h, y)\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "class ConvMeanPool(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True, adjust_padding=False):\n",
    "    super().__init__()\n",
    "    if not adjust_padding:\n",
    "      conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n",
    "      self.conv = conv\n",
    "    else:\n",
    "      conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n",
    "\n",
    "      self.conv = nn.Sequential(\n",
    "        nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "        conv\n",
    "      )\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    output = self.conv(inputs)\n",
    "    output = sum([output[:, :, ::2, ::2], output[:, :, 1::2, ::2],\n",
    "                  output[:, :, ::2, 1::2], output[:, :, 1::2, 1::2]]) / 4.\n",
    "    return output\n",
    "\n",
    "\n",
    "class MeanPoolConv(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    output = inputs\n",
    "    output = sum([output[:, :, ::2, ::2], output[:, :, 1::2, ::2],\n",
    "                  output[:, :, ::2, 1::2], output[:, :, 1::2, 1::2]]) / 4.\n",
    "    return self.conv(output)\n",
    "\n",
    "\n",
    "class UpsampleConv(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n",
    "    self.pixelshuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    output = inputs\n",
    "    output = torch.cat([output, output, output, output], dim=1)\n",
    "    output = self.pixelshuffle(output)\n",
    "    return self.conv(output)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, num_classes, resample=1, act=nn.ELU(),\n",
    "               normalization=ConditionalInstanceNorm2dPlus, adjust_padding=False, dilation=None):\n",
    "    super().__init__()\n",
    "    self.non_linearity = act\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.resample = resample\n",
    "    self.normalization = normalization\n",
    "    if resample == 'down':\n",
    "      if dilation > 1:\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, input_dim, dilation=dilation)\n",
    "        self.normalize2 = normalization(input_dim, num_classes)\n",
    "        self.conv2 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n",
    "        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n",
    "      else:\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, input_dim)\n",
    "        self.normalize2 = normalization(input_dim, num_classes)\n",
    "        self.conv2 = ConvMeanPool(input_dim, output_dim, 3, adjust_padding=adjust_padding)\n",
    "        conv_shortcut = partial(ConvMeanPool, kernel_size=1, adjust_padding=adjust_padding)\n",
    "\n",
    "    elif resample is None:\n",
    "      if dilation > 1:\n",
    "        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n",
    "        self.normalize2 = normalization(output_dim, num_classes)\n",
    "        self.conv2 = ncsn_conv3x3(output_dim, output_dim, dilation=dilation)\n",
    "      else:\n",
    "        conv_shortcut = nn.Conv2d\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, output_dim)\n",
    "        self.normalize2 = normalization(output_dim, num_classes)\n",
    "        self.conv2 = ncsn_conv3x3(output_dim, output_dim)\n",
    "    else:\n",
    "      raise Exception('invalid resample value')\n",
    "\n",
    "    if output_dim != input_dim or resample is not None:\n",
    "      self.shortcut = conv_shortcut(input_dim, output_dim)\n",
    "\n",
    "    self.normalize1 = normalization(input_dim, num_classes)\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    output = self.normalize1(x, y)\n",
    "    output = self.non_linearity(output)\n",
    "    output = self.conv1(output)\n",
    "    output = self.normalize2(output, y)\n",
    "    output = self.non_linearity(output)\n",
    "    output = self.conv2(output)\n",
    "\n",
    "    if self.output_dim == self.input_dim and self.resample is None:\n",
    "      shortcut = x\n",
    "    else:\n",
    "      shortcut = self.shortcut(x)\n",
    "\n",
    "    return shortcut + output\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, resample=None, act=nn.ELU(),\n",
    "               normalization=nn.InstanceNorm2d, adjust_padding=False, dilation=1):\n",
    "    super().__init__()\n",
    "    self.non_linearity = act\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.resample = resample\n",
    "    self.normalization = normalization\n",
    "    if resample == 'down':\n",
    "      if dilation > 1:\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, input_dim, dilation=dilation)\n",
    "        self.normalize2 = normalization(input_dim)\n",
    "        self.conv2 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n",
    "        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n",
    "      else:\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, input_dim)\n",
    "        self.normalize2 = normalization(input_dim)\n",
    "        self.conv2 = ConvMeanPool(input_dim, output_dim, 3, adjust_padding=adjust_padding)\n",
    "        conv_shortcut = partial(ConvMeanPool, kernel_size=1, adjust_padding=adjust_padding)\n",
    "\n",
    "    elif resample is None:\n",
    "      if dilation > 1:\n",
    "        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n",
    "        self.normalize2 = normalization(output_dim)\n",
    "        self.conv2 = ncsn_conv3x3(output_dim, output_dim, dilation=dilation)\n",
    "      else:\n",
    "        # conv_shortcut = nn.Conv2d ### Something wierd here.\n",
    "        conv_shortcut = partial(ncsn_conv1x1)\n",
    "        self.conv1 = ncsn_conv3x3(input_dim, output_dim)\n",
    "        self.normalize2 = normalization(output_dim)\n",
    "        self.conv2 = ncsn_conv3x3(output_dim, output_dim)\n",
    "    else:\n",
    "      raise Exception('invalid resample value')\n",
    "\n",
    "    if output_dim != input_dim or resample is not None:\n",
    "      self.shortcut = conv_shortcut(input_dim, output_dim)\n",
    "\n",
    "    self.normalize1 = normalization(input_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.normalize1(x)\n",
    "    output = self.non_linearity(output)\n",
    "    output = self.conv1(output)\n",
    "    output = self.normalize2(output)\n",
    "    output = self.non_linearity(output)\n",
    "    output = self.conv2(output)\n",
    "\n",
    "    if self.output_dim == self.input_dim and self.resample is None:\n",
    "      shortcut = x\n",
    "    else:\n",
    "      shortcut = self.shortcut(x)\n",
    "\n",
    "    return shortcut + output\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# Functions below are ported over from the DDPM codebase:\n",
    "#  https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py\n",
    "###########################################################################\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n",
    "  assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n",
    "  half_dim = embedding_dim // 2\n",
    "  # magic number 10000 is from transformers\n",
    "  emb = math.log(max_positions) / (half_dim - 1)\n",
    "  # emb = math.log(2.) / (half_dim - 1)\n",
    "  emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "  # emb = tf.range(num_embeddings, dtype=jnp.float32)[:, None] * emb[None, :]\n",
    "  # emb = tf.cast(timesteps, dtype=jnp.float32)[:, None] * emb[None, :]\n",
    "  emb = timesteps.float()[:, None] * emb[None, :]\n",
    "  emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "  if embedding_dim % 2 == 1:  # zero pad\n",
    "    emb = F.pad(emb, (0, 1), mode='constant')\n",
    "  assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "  return emb\n",
    "\n",
    "\n",
    "def _einsum(a, b, c, x, y):\n",
    "  einsum_str = '{},{}->{}'.format(''.join(a), ''.join(b), ''.join(c))\n",
    "  return torch.einsum(einsum_str, x, y)\n",
    "\n",
    "\n",
    "def contract_inner(x, y):\n",
    "  \"\"\"tensordot(x, y, 1).\"\"\"\n",
    "  x_chars = list(string.ascii_lowercase[:len(x.shape)])\n",
    "  y_chars = list(string.ascii_lowercase[len(x.shape):len(y.shape) + len(x.shape)])\n",
    "  y_chars[0] = x_chars[-1]  # first axis of y and last of x get summed\n",
    "  out_chars = x_chars[:-1] + y_chars[1:]\n",
    "  return _einsum(x_chars, y_chars, out_chars, x, y)\n",
    "\n",
    "\n",
    "class NIN(nn.Module):\n",
    "  def __init__(self, in_dim, num_units, init_scale=0.1):\n",
    "    super().__init__()\n",
    "    self.W = nn.Parameter(default_init(scale=init_scale)((in_dim, num_units)), requires_grad=True)\n",
    "    self.b = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    y = contract_inner(x, self.W) + self.b\n",
    "    return y.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "  \"\"\"Channel-wise self-attention block.\"\"\"\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n",
    "    self.NIN_0 = NIN(channels, channels)\n",
    "    self.NIN_1 = NIN(channels, channels)\n",
    "    self.NIN_2 = NIN(channels, channels)\n",
    "    self.NIN_3 = NIN(channels, channels, init_scale=0.)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, C, H, W = x.shape\n",
    "    h = self.GroupNorm_0(x)\n",
    "    q = self.NIN_0(h)\n",
    "    k = self.NIN_1(h)\n",
    "    v = self.NIN_2(h)\n",
    "\n",
    "    w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n",
    "    w = torch.reshape(w, (B, H, W, H * W))\n",
    "    w = F.softmax(w, dim=-1)\n",
    "    w = torch.reshape(w, (B, H, W, H, W))\n",
    "    h = torch.einsum('bhwij,bcij->bchw', w, v)\n",
    "    h = self.NIN_3(h)\n",
    "    return x + h\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "  def __init__(self, channels, with_conv=False):\n",
    "    super().__init__()\n",
    "    if with_conv:\n",
    "      self.Conv_0 = ddpm_conv3x3(channels, channels)\n",
    "    self.with_conv = with_conv\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, C, H, W = x.shape\n",
    "    h = F.interpolate(x, (H * 2, W * 2), mode='nearest')\n",
    "    if self.with_conv:\n",
    "      h = self.Conv_0(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "  def __init__(self, channels, with_conv=False):\n",
    "    super().__init__()\n",
    "    if with_conv:\n",
    "      self.Conv_0 = ddpm_conv3x3(channels, channels, stride=2, padding=0)\n",
    "    self.with_conv = with_conv\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, C, H, W = x.shape\n",
    "    # Emulate 'SAME' padding\n",
    "    if self.with_conv:\n",
    "      x = F.pad(x, (0, 1, 0, 1))\n",
    "      x = self.Conv_0(x)\n",
    "    else:\n",
    "      x = F.avg_pool2d(x, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    assert x.shape == (B, C, H // 2, W // 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "class ResnetBlockDDPM(nn.Module):\n",
    "  \"\"\"The ResNet Blocks used in DDPM.\"\"\"\n",
    "  def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1):\n",
    "    super().__init__()\n",
    "    if out_ch is None:\n",
    "      out_ch = in_ch\n",
    "    self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=in_ch, eps=1e-6)\n",
    "    self.act = act\n",
    "    self.Conv_0 = ddpm_conv3x3(in_ch, out_ch)\n",
    "    if temb_dim is not None:\n",
    "      self.Dense_0 = nn.Linear(temb_dim, out_ch)\n",
    "      self.Dense_0.weight.data = default_init()(self.Dense_0.weight.data.shape)\n",
    "      nn.init.zeros_(self.Dense_0.bias)\n",
    "\n",
    "    self.GroupNorm_1 = nn.GroupNorm(num_groups=32, num_channels=out_ch, eps=1e-6)\n",
    "    self.Dropout_0 = nn.Dropout(dropout)\n",
    "    self.Conv_1 = ddpm_conv3x3(out_ch, out_ch, init_scale=0.)\n",
    "    if in_ch != out_ch:\n",
    "      if conv_shortcut:\n",
    "        self.Conv_2 = ddpm_conv3x3(in_ch, out_ch)\n",
    "      else:\n",
    "        self.NIN_0 = NIN(in_ch, out_ch)\n",
    "    self.out_ch = out_ch\n",
    "    self.in_ch = in_ch\n",
    "    self.conv_shortcut = conv_shortcut\n",
    "\n",
    "  def forward(self, x, temb=None):\n",
    "    B, C, H, W = x.shape\n",
    "    assert C == self.in_ch\n",
    "    out_ch = self.out_ch if self.out_ch else self.in_ch\n",
    "    h = self.act(self.GroupNorm_0(x))\n",
    "    h = self.Conv_0(h)\n",
    "    # Add bias to each feature map conditioned on the time embedding\n",
    "    if temb is not None:\n",
    "      h += self.Dense_0(self.act(temb))[:, :, None, None]\n",
    "    h = self.act(self.GroupNorm_1(h))\n",
    "    h = self.Dropout_0(h)\n",
    "    h = self.Conv_1(h)\n",
    "    if C != out_ch:\n",
    "      if self.conv_shortcut:\n",
    "        x = self.Conv_2(x)\n",
    "      else:\n",
    "        x = self.NIN_0(x)\n",
    "    return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde56164-63ad-4e66-9c07-564f9063a703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "\n",
    "def get_normalization(config, conditional=False):\n",
    "  \"\"\"Obtain normalization modules from the config file.\"\"\"\n",
    "  norm = config.model.normalization\n",
    "  if conditional:\n",
    "    if norm == 'InstanceNorm++':\n",
    "      return functools.partial(ConditionalInstanceNorm2dPlus, num_classes=config.model.num_classes)\n",
    "    else:\n",
    "      raise NotImplementedError(f'{norm} not implemented yet.')\n",
    "  else:\n",
    "    if norm == 'InstanceNorm':\n",
    "      return nn.InstanceNorm2d\n",
    "    elif norm == 'InstanceNorm++':\n",
    "      return InstanceNorm2dPlus\n",
    "    elif norm == 'VarianceNorm':\n",
    "      return VarianceNorm2d\n",
    "    elif norm == 'GroupNorm':\n",
    "      return nn.GroupNorm\n",
    "    else:\n",
    "      raise ValueError('Unknown normalization: %s' % norm)\n",
    "\n",
    "conv3x3 = ddpm_conv3x3\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.act = act = get_act(config)\n",
    "    self.register_buffer('sigmas', torch.tensor(utils.get_sigmas(config)))\n",
    "\n",
    "    self.nf = nf = config.model.nf\n",
    "    ch_mult = config.model.ch_mult\n",
    "    self.num_res_blocks = num_res_blocks = config.model.num_res_blocks\n",
    "    self.attn_resolutions = attn_resolutions = config.model.attn_resolutions\n",
    "    dropout = config.model.dropout\n",
    "    resamp_with_conv = config.model.resamp_with_conv\n",
    "    self.num_resolutions = num_resolutions = len(ch_mult)\n",
    "    self.all_resolutions = all_resolutions = [config.data.image_size // (2 ** i) for i in range(num_resolutions)]\n",
    "\n",
    "    AttnBlock = functools.partial(layers.AttnBlock)\n",
    "    self.conditional = conditional = config.model.conditional\n",
    "    ResnetBlock = functools.partial(ResnetBlockDDPM, act=act, temb_dim=4 * nf, dropout=dropout)\n",
    "    if conditional:\n",
    "      # Condition on noise levels.\n",
    "      modules = [nn.Linear(nf, nf * 4)]\n",
    "      modules[0].weight.data = default_initializer()(modules[0].weight.data.shape)\n",
    "      nn.init.zeros_(modules[0].bias)\n",
    "      modules.append(nn.Linear(nf * 4, nf * 4))\n",
    "      modules[1].weight.data = default_initializer()(modules[1].weight.data.shape)\n",
    "      nn.init.zeros_(modules[1].bias)\n",
    "\n",
    "    self.centered = config.data.centered\n",
    "    channels = config.data.num_channels\n",
    "\n",
    "    # Downsampling block\n",
    "    modules.append(conv3x3(channels, nf))\n",
    "    hs_c = [nf]\n",
    "    in_ch = nf\n",
    "    for i_level in range(num_resolutions):\n",
    "      # Residual blocks for this resolution\n",
    "      for i_block in range(num_res_blocks):\n",
    "        out_ch = nf * ch_mult[i_level]\n",
    "        modules.append(ResnetBlock(in_ch=in_ch, out_ch=out_ch))\n",
    "        in_ch = out_ch\n",
    "        if all_resolutions[i_level] in attn_resolutions:\n",
    "          modules.append(AttnBlock(channels=in_ch))\n",
    "        hs_c.append(in_ch)\n",
    "      if i_level != num_resolutions - 1:\n",
    "        modules.append(Downsample(channels=in_ch, with_conv=resamp_with_conv))\n",
    "        hs_c.append(in_ch)\n",
    "\n",
    "    in_ch = hs_c[-1]\n",
    "    modules.append(ResnetBlock(in_ch=in_ch))\n",
    "    modules.append(AttnBlock(channels=in_ch))\n",
    "    modules.append(ResnetBlock(in_ch=in_ch))\n",
    "\n",
    "    # Upsampling block\n",
    "    for i_level in reversed(range(num_resolutions)):\n",
    "      for i_block in range(num_res_blocks + 1):\n",
    "        out_ch = nf * ch_mult[i_level]\n",
    "        modules.append(ResnetBlock(in_ch=in_ch + hs_c.pop(), out_ch=out_ch))\n",
    "        in_ch = out_ch\n",
    "      if all_resolutions[i_level] in attn_resolutions:\n",
    "        modules.append(AttnBlock(channels=in_ch))\n",
    "      if i_level != 0:\n",
    "        modules.append(Upsample(channels=in_ch, with_conv=resamp_with_conv))\n",
    "\n",
    "    assert not hs_c\n",
    "    modules.append(nn.GroupNorm(num_channels=in_ch, num_groups=32, eps=1e-6))\n",
    "    modules.append(conv3x3(in_ch, channels, init_scale=0.))\n",
    "    self.all_modules = nn.ModuleList(modules)\n",
    "\n",
    "    self.scale_by_sigma = config.model.scale_by_sigma\n",
    "\n",
    "  def forward(self, x, labels):\n",
    "    modules = self.all_modules\n",
    "    m_idx = 0\n",
    "    if self.conditional:\n",
    "      # timestep/scale embedding\n",
    "      timesteps = labels\n",
    "      temb = layers.get_timestep_embedding(timesteps, self.nf)\n",
    "      temb = modules[m_idx](temb)\n",
    "      m_idx += 1\n",
    "      temb = modules[m_idx](self.act(temb))\n",
    "      m_idx += 1\n",
    "    else:\n",
    "      temb = None\n",
    "\n",
    "    if self.centered:\n",
    "      # Input is in [-1, 1]\n",
    "      h = x\n",
    "    else:\n",
    "      # Input is in [0, 1]\n",
    "      h = 2 * x - 1.\n",
    "\n",
    "    # Downsampling block\n",
    "    hs = [modules[m_idx](h)]\n",
    "    m_idx += 1\n",
    "    for i_level in range(self.num_resolutions):\n",
    "      # Residual blocks for this resolution\n",
    "      for i_block in range(self.num_res_blocks):\n",
    "        h = modules[m_idx](hs[-1], temb)\n",
    "        m_idx += 1\n",
    "        if h.shape[-1] in self.attn_resolutions:\n",
    "          h = modules[m_idx](h)\n",
    "          m_idx += 1\n",
    "        hs.append(h)\n",
    "      if i_level != self.num_resolutions - 1:\n",
    "        hs.append(modules[m_idx](hs[-1]))\n",
    "        m_idx += 1\n",
    "\n",
    "    h = hs[-1]\n",
    "    h = modules[m_idx](h, temb)\n",
    "    m_idx += 1\n",
    "    h = modules[m_idx](h)\n",
    "    m_idx += 1\n",
    "    h = modules[m_idx](h, temb)\n",
    "    m_idx += 1\n",
    "\n",
    "    # Upsampling block\n",
    "    for i_level in reversed(range(self.num_resolutions)):\n",
    "      for i_block in range(self.num_res_blocks + 1):\n",
    "        h = modules[m_idx](torch.cat([h, hs.pop()], dim=1), temb)\n",
    "        m_idx += 1\n",
    "      if h.shape[-1] in self.attn_resolutions:\n",
    "        h = modules[m_idx](h)\n",
    "        m_idx += 1\n",
    "      if i_level != 0:\n",
    "        h = modules[m_idx](h)\n",
    "        m_idx += 1\n",
    "\n",
    "    assert not hs\n",
    "    h = self.act(modules[m_idx](h))\n",
    "    m_idx += 1\n",
    "    h = modules[m_idx](h)\n",
    "    m_idx += 1\n",
    "    assert m_idx == len(modules)\n",
    "\n",
    "    if self.scale_by_sigma:\n",
    "      # Divide the output by sigmas. Useful for training with the NCSN loss.\n",
    "      # The DDPM loss scales the network output by sigma in the loss function,\n",
    "      # so no need of doing it here.\n",
    "      used_sigmas = self.sigmas[labels, None, None, None]\n",
    "      h = h / used_sigmas\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10462656-116c-4129-9753-e6a42397bac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_default_configs():\n",
    "  config = ml_collections.ConfigDict()\n",
    "  # training\n",
    "  config.training = training = ml_collections.ConfigDict()\n",
    "  config.training.batch_size = 128\n",
    "  training.n_iters = 1300001\n",
    "  training.snapshot_freq = 50000\n",
    "  training.log_freq = 50\n",
    "  training.eval_freq = 100\n",
    "  ## store additional checkpoints for preemption in cloud computing environments\n",
    "  training.snapshot_freq_for_preemption = 10000\n",
    "  ## produce samples at each snapshot.\n",
    "  training.snapshot_sampling = True\n",
    "  training.likelihood_weighting = False\n",
    "  training.continuous = False\n",
    "  training.reduce_mean = False\n",
    "\n",
    "  # sampling\n",
    "  config.sampling = sampling = ml_collections.ConfigDict()\n",
    "  sampling.n_steps_each = 1\n",
    "  sampling.noise_removal = True\n",
    "  sampling.probability_flow = False\n",
    "  sampling.snr = 0.17\n",
    "\n",
    "  # evaluation\n",
    "  config.eval = evaluate = ml_collections.ConfigDict()\n",
    "  evaluate.begin_ckpt = 1\n",
    "  evaluate.end_ckpt = 26\n",
    "  evaluate.batch_size = 1024\n",
    "  evaluate.enable_sampling = True\n",
    "  evaluate.num_samples = 50000\n",
    "  evaluate.enable_loss = True\n",
    "  evaluate.enable_bpd = False\n",
    "  evaluate.bpd_dataset = 'test'\n",
    "\n",
    "  # data\n",
    "  config.data = data = ml_collections.ConfigDict()\n",
    "  data.dataset = 'DungeonDiffusion'\n",
    "  data.image_size = 512\n",
    "  data.random_flip = True\n",
    "  data.uniform_dequantization = False\n",
    "  data.centered = False\n",
    "  data.num_channels = 3\n",
    "\n",
    "  # model\n",
    "  config.model = model = ml_collections.ConfigDict()\n",
    "  model.sigma_max = 90.\n",
    "  model.sigma_min = 0.01\n",
    "  model.num_scales = 1000\n",
    "  model.beta_min = 0.1\n",
    "  model.beta_max = 20.\n",
    "  model.dropout = 0.1\n",
    "  model.embedding_type = 'fourier'\n",
    "\n",
    "  # optimization\n",
    "  config.optim = optim = ml_collections.ConfigDict()\n",
    "  optim.weight_decay = 0\n",
    "  optim.optimizer = 'Adam'\n",
    "  optim.lr = 2e-4\n",
    "  optim.beta1 = 0.9\n",
    "  optim.eps = 1e-8\n",
    "  optim.warmup = 5000\n",
    "  optim.grad_clip = 1.\n",
    "\n",
    "  config.seed = 42\n",
    "  config.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "  return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3719e-25cd-4de9-b36d-34a99ec5a8f4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38577e20-b3f2-4fc4-b859-473e27b6d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model.\n",
    "score_model = DDPM(get_default_configs())\n",
    "ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n",
    "optimizer = losses.get_optimizer(config, score_model.parameters())\n",
    "state = dict(optimizer=optimizer, model=score_model, ema=ema, step=0)\n",
    "\n",
    "# Create checkpoints directory\n",
    "#checkpoint_dir = os.path.join(workdir, \"checkpoints\")\n",
    "# Intermediate checkpoints to resume training after pre-emption in cloud environments\n",
    "#checkpoint_meta_dir = os.path.join(workdir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "#tf.io.gfile.makedirs(checkpoint_dir)\n",
    "#tf.io.gfile.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "# Resume training when intermediate checkpoints are detected\n",
    "#state = restore_checkpoint(checkpoint_meta_dir, state, config.device)\n",
    "initial_step = int(state['step'])\n",
    "\n",
    "# Build data iterators\n",
    "#train_ds, eval_ds, _ = datasets.get_dataset(config,\n",
    "#                                          uniform_dequantization=config.data.uniform_dequantization)\n",
    "#train_iter = iter(train_ds)  # pytype: disable=wrong-arg-types\n",
    "#eval_iter = iter(eval_ds)  # pytype: disable=wrong-arg-types\n",
    "# Create data normalizer and its inverse\n",
    "#scaler = datasets.get_data_scaler(config)\n",
    "#inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
    "\n",
    "# Setup SDEs\n",
    "sde = VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n",
    "sampling_eps = 1e-3\n",
    "\n",
    "# Build one-step training and evaluation functions\n",
    "optimize_fn = losses.optimization_manager(config)\n",
    "continuous = config.training.continuous\n",
    "reduce_mean = config.training.reduce_mean\n",
    "likelihood_weighting = config.training.likelihood_weighting\n",
    "train_step_fn = losses.get_step_fn(sde, train=True, optimize_fn=optimize_fn,\n",
    "                                 reduce_mean=reduce_mean, continuous=continuous,\n",
    "                                 likelihood_weighting=likelihood_weighting)\n",
    "eval_step_fn = losses.get_step_fn(sde, train=False, optimize_fn=optimize_fn,\n",
    "                                reduce_mean=reduce_mean, continuous=continuous,\n",
    "                                likelihood_weighting=likelihood_weighting)\n",
    "\n",
    "# Building sampling functions\n",
    "#if config.training.snapshot_sampling:\n",
    "#    sampling_shape = (config.training.batch_size, config.data.num_channels,\n",
    "#                      config.data.image_size, config.data.image_size)\n",
    "#    sampling_fn = sampling.get_sampling_fn(config, sde, sampling_shape, inverse_scaler, sampling_eps)\n",
    "\n",
    "num_train_steps = config.training.n_iters\n",
    "\n",
    "# In case there are multiple hosts (e.g., TPU pods), only log to host 0\n",
    "logging.info(\"Starting training loop at step %d.\" % (initial_step,))\n",
    "\n",
    "for step in range(initial_step, num_train_steps + 1):\n",
    "# Convert data to JAX arrays and normalize them. Use ._numpy() to avoid copy.\n",
    "batch = torch.from_numpy(next(train_iter)['image']._numpy()).to(config.device).float()\n",
    "batch = batch.permute(0, 3, 1, 2)\n",
    "batch = scaler(batch)\n",
    "# Execute one training step\n",
    "loss = train_step_fn(state, batch)\n",
    "if step % config.training.log_freq == 0:\n",
    "  print(\"step: %d, training_loss: %.5e\" % (step, loss.item()))\n",
    "  \n",
    "\n",
    "# Save a temporary checkpoint to resume training after pre-emption periodically\n",
    "#if step != 0 and step % config.training.snapshot_freq_for_preemption == 0:\n",
    "#  save_checkpoint(checkpoint_meta_dir, state)\n",
    "\n",
    "# Report the loss on an evaluation dataset periodically\n",
    "#if step % config.training.eval_freq == 0:\n",
    "#  eval_batch = torch.from_numpy(next(eval_iter)['image']._numpy()).to(config.device).float()\n",
    "#  eval_batch = eval_batch.permute(0, 3, 1, 2)\n",
    "#  eval_batch = scaler(eval_batch)\n",
    "#  eval_loss = eval_step_fn(state, eval_batch)\n",
    "#  logging.info(\"step: %d, eval_loss: %.5e\" % (step, eval_loss.item()))\n",
    "#  writer.add_scalar(\"eval_loss\", eval_loss.item(), step)\n",
    "\n",
    "# Save a checkpoint periodically and generate samples if needed\n",
    "#if step != 0 and step % config.training.snapshot_freq == 0 or step == num_train_steps:\n",
    "  # Save the checkpoint.\n",
    "#  save_step = step // config.training.snapshot_freq\n",
    "#  save_checkpoint(os.path.join(checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n",
    "\n",
    "  # Generate and save samples\n",
    "#  if config.training.snapshot_sampling:\n",
    "#    ema.store(score_model.parameters())\n",
    "#    ema.copy_to(score_model.parameters())\n",
    "#    sample, n = sampling_fn(score_model)\n",
    "#    ema.restore(score_model.parameters())\n",
    "#    this_sample_dir = os.path.join(sample_dir, \"iter_{}\".format(step))\n",
    "#    tf.io.gfile.makedirs(this_sample_dir)\n",
    "#    nrow = int(np.sqrt(sample.shape[0]))\n",
    "#    image_grid = make_grid(sample, nrow, padding=2)\n",
    "#    sample = np.clip(sample.permute(0, 2, 3, 1).cpu().numpy() * 255, 0, 255).astype(np.uint8)\n",
    "#    with tf.io.gfile.GFile(\n",
    "#        os.path.join(this_sample_dir, \"sample.np\"), \"wb\") as fout:\n",
    "#      np.save(fout, sample)\n",
    "\n",
    "#    with tf.io.gfile.GFile(\n",
    "#        os.path.join(this_sample_dir, \"sample.png\"), \"wb\") as fout:\n",
    "#      save_image(image_grid, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
