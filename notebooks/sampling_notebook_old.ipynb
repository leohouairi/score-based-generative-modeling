{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6296d-86a4-4a95-8ca0-7bee660a8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from models.ema import ExponentialMovingAverage\n",
    "import likelihood\n",
    "import controllable_generation\n",
    "from utils import restore_checkpoint\n",
    "\n",
    "import models\n",
    "from models import utils as mutils\n",
    "from models import ncsnv2\n",
    "from models import ncsnpp\n",
    "from models import ddpm as ddpm_model\n",
    "from models import layerspp\n",
    "from models import layers\n",
    "from models import normalization\n",
    "import sampling\n",
    "from likelihood import get_likelihood_fn\n",
    "from sampling import (ReverseDiffusionPredictor, \n",
    "                      LangevinCorrector, \n",
    "                      EulerMaruyamaPredictor, \n",
    "                      AncestralSamplingPredictor, \n",
    "                      NoneCorrector, \n",
    "                      NonePredictor,\n",
    "                      AnnealedLangevinDynamics)\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa49471-f76f-481b-99b9-d7331822a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yang Song sde_lib.py\n",
    "\n",
    "\"\"\"Abstract SDE classes, Reverse SDE, and VE/VP SDEs.\"\"\"\n",
    "import abc\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SDE(abc.ABC):\n",
    "  \"\"\"SDE abstract class. Functions are designed for a mini-batch of inputs.\"\"\"\n",
    "\n",
    "  def __init__(self, N):\n",
    "    \"\"\"Construct an SDE.\n",
    "\n",
    "    Args:\n",
    "      N: number of discretization time steps.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.N = N\n",
    "\n",
    "  @property\n",
    "  @abc.abstractmethod\n",
    "  def T(self):\n",
    "    \"\"\"End time of the SDE.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def sde(self, x, t):\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def marginal_prob(self, x, t):\n",
    "    \"\"\"Parameters to determine the marginal distribution of the SDE, $p_t(x)$.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def prior_sampling(self, shape):\n",
    "    \"\"\"Generate one sample from the prior distribution, $p_T(x)$.\"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def prior_logp(self, z):\n",
    "    \"\"\"Compute log-density of the prior distribution.\n",
    "\n",
    "    Useful for computing the log-likelihood via probability flow ODE.\n",
    "\n",
    "    Args:\n",
    "      z: latent code\n",
    "    Returns:\n",
    "      log probability density\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def discretize(self, x, t):\n",
    "    \"\"\"Discretize the SDE in the form: x_{i+1} = x_i + f_i(x_i) + G_i z_i.\n",
    "\n",
    "    Useful for reverse diffusion sampling and probabiliy flow sampling.\n",
    "    Defaults to Euler-Maruyama discretization.\n",
    "\n",
    "    Args:\n",
    "      x: a torch tensor\n",
    "      t: a torch float representing the time step (from 0 to `self.T`)\n",
    "\n",
    "    Returns:\n",
    "      f, G\n",
    "    \"\"\"\n",
    "    dt = 1 / self.N\n",
    "    drift, diffusion = self.sde(x, t)\n",
    "    f = drift * dt\n",
    "    G = diffusion * torch.sqrt(torch.tensor(dt, device=t.device))\n",
    "    return f, G\n",
    "\n",
    "  def reverse(self, score_fn, probability_flow=False):\n",
    "    \"\"\"Create the reverse-time SDE/ODE.\n",
    "\n",
    "    Args:\n",
    "      score_fn: A time-dependent score-based model that takes x and t and returns the score.\n",
    "      probability_flow: If `True`, create the reverse-time ODE used for probability flow sampling.\n",
    "    \"\"\"\n",
    "    N = self.N\n",
    "    T = self.T\n",
    "    sde_fn = self.sde\n",
    "    discretize_fn = self.discretize\n",
    "\n",
    "    # Build the class for reverse-time SDE.\n",
    "    class RSDE(self.__class__):\n",
    "      def __init__(self):\n",
    "        self.N = N\n",
    "        self.probability_flow = probability_flow\n",
    "\n",
    "      @property\n",
    "      def T(self):\n",
    "        return T\n",
    "\n",
    "      def sde(self, x, t):\n",
    "        \"\"\"Create the drift and diffusion functions for the reverse SDE/ODE.\"\"\"\n",
    "        drift, diffusion = sde_fn(x, t)\n",
    "        score = score_fn(x, t)\n",
    "        drift = drift - diffusion[:, None, None, None] ** 2 * score * (0.5 if self.probability_flow else 1.)\n",
    "        # Set the diffusion function to zero for ODEs.\n",
    "        diffusion = 0. if self.probability_flow else diffusion\n",
    "        return drift, diffusion\n",
    "\n",
    "      def discretize(self, x, t):\n",
    "        \"\"\"Create discretized iteration rules for the reverse diffusion sampler.\"\"\"\n",
    "        f, G = discretize_fn(x, t)\n",
    "        rev_f = f - G[:, None, None, None] ** 2 * score_fn(x, t) * (0.5 if self.probability_flow else 1.)\n",
    "        rev_G = torch.zeros_like(G) if self.probability_flow else G\n",
    "        return rev_f, rev_G\n",
    "\n",
    "    return RSDE()\n",
    "\n",
    "\n",
    "class VPSDE(SDE):\n",
    "  def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n",
    "    \"\"\"Construct a Variance Preserving SDE.\n",
    "\n",
    "    Args:\n",
    "      beta_min: value of beta(0)\n",
    "      beta_max: value of beta(1)\n",
    "      N: number of discretization steps\n",
    "    \"\"\"\n",
    "    super().__init__(N)\n",
    "    self.beta_0 = beta_min\n",
    "    self.beta_1 = beta_max\n",
    "    self.N = N\n",
    "    self.discrete_betas = torch.linspace(beta_min / N, beta_max / N, N)\n",
    "    self.alphas = 1. - self.discrete_betas\n",
    "    self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "    self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "    self.sqrt_1m_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "\n",
    "  @property\n",
    "  def T(self):\n",
    "    return 1\n",
    "\n",
    "  def sde(self, x, t):\n",
    "    beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n",
    "    drift = -0.5 * beta_t[:, None, None, None] * x\n",
    "    diffusion = torch.sqrt(beta_t)\n",
    "    return drift, diffusion\n",
    "\n",
    "  def marginal_prob(self, x, t):\n",
    "    log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n",
    "    mean = torch.exp(log_mean_coeff[:, None, None, None]) * x\n",
    "    std = torch.sqrt(1. - torch.exp(2. * log_mean_coeff))\n",
    "    return mean, std\n",
    "\n",
    "  def prior_sampling(self, shape):\n",
    "    return torch.randn(*shape)\n",
    "\n",
    "  def prior_logp(self, z):\n",
    "    shape = z.shape\n",
    "    N = np.prod(shape[1:])\n",
    "    logps = -N / 2. * np.log(2 * np.pi) - torch.sum(z ** 2, dim=(1, 2, 3)) / 2.\n",
    "    return logps\n",
    "\n",
    "  def discretize(self, x, t):\n",
    "    \"\"\"DDPM discretization.\"\"\"\n",
    "    timestep = (t * (self.N - 1) / self.T).long()\n",
    "    beta = self.discrete_betas.to(x.device)[timestep]\n",
    "    alpha = self.alphas.to(x.device)[timestep]\n",
    "    sqrt_beta = torch.sqrt(beta)\n",
    "    f = torch.sqrt(alpha)[:, None, None, None] * x - x\n",
    "    G = sqrt_beta\n",
    "    return f, G\n",
    "\n",
    "\n",
    "class subVPSDE(SDE):\n",
    "  def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n",
    "    \"\"\"Construct the sub-VP SDE that excels at likelihoods.\n",
    "\n",
    "    Args:\n",
    "      beta_min: value of beta(0)\n",
    "      beta_max: value of beta(1)\n",
    "      N: number of discretization steps\n",
    "    \"\"\"\n",
    "    super().__init__(N)\n",
    "    self.beta_0 = beta_min\n",
    "    self.beta_1 = beta_max\n",
    "    self.N = N\n",
    "\n",
    "  @property\n",
    "  def T(self):\n",
    "    return 1\n",
    "\n",
    "  def sde(self, x, t):\n",
    "    beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n",
    "    drift = -0.5 * beta_t[:, None, None, None] * x\n",
    "    discount = 1. - torch.exp(-2 * self.beta_0 * t - (self.beta_1 - self.beta_0) * t ** 2)\n",
    "    diffusion = torch.sqrt(beta_t * discount)\n",
    "    return drift, diffusion\n",
    "\n",
    "  def marginal_prob(self, x, t):\n",
    "    log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n",
    "    mean = torch.exp(log_mean_coeff)[:, None, None, None] * x\n",
    "    std = 1 - torch.exp(2. * log_mean_coeff)\n",
    "    return mean, std\n",
    "\n",
    "  def prior_sampling(self, shape):\n",
    "    return torch.randn(*shape)\n",
    "\n",
    "  def prior_logp(self, z):\n",
    "    shape = z.shape\n",
    "    N = np.prod(shape[1:])\n",
    "    return -N / 2. * np.log(2 * np.pi) - torch.sum(z ** 2, dim=(1, 2, 3)) / 2.\n",
    "\n",
    "\n",
    "class VESDE(SDE):\n",
    "  def __init__(self, sigma_min=0.01, sigma_max=50, N=1000):\n",
    "    \"\"\"Construct a Variance Exploding SDE.\n",
    "\n",
    "    Args:\n",
    "      sigma_min: smallest sigma.\n",
    "      sigma_max: largest sigma.\n",
    "      N: number of discretization steps\n",
    "    \"\"\"\n",
    "    super().__init__(N)\n",
    "    self.sigma_min = sigma_min\n",
    "    self.sigma_max = sigma_max\n",
    "    self.discrete_sigmas = torch.exp(torch.linspace(np.log(self.sigma_min), np.log(self.sigma_max), N))\n",
    "    self.N = N\n",
    "\n",
    "  @property\n",
    "  def T(self):\n",
    "    return 1\n",
    "\n",
    "  def sde(self, x, t):\n",
    "    sigma = self.sigma_min * (self.sigma_max / self.sigma_min) ** t\n",
    "    drift = torch.zeros_like(x)\n",
    "    diffusion = sigma * torch.sqrt(torch.tensor(2 * (np.log(self.sigma_max) - np.log(self.sigma_min)),\n",
    "                                                device=t.device))\n",
    "    return drift, diffusion\n",
    "\n",
    "  def marginal_prob(self, x, t):\n",
    "    std = self.sigma_min * (self.sigma_max / self.sigma_min) ** t\n",
    "    mean = x\n",
    "    return mean, std\n",
    "\n",
    "  def prior_sampling(self, shape):\n",
    "    return torch.randn(*shape) * self.sigma_max\n",
    "\n",
    "  def prior_logp(self, z):\n",
    "    shape = z.shape\n",
    "    N = np.prod(shape[1:])\n",
    "    return -N / 2. * np.log(2 * np.pi * self.sigma_max ** 2) - torch.sum(z ** 2, dim=(1, 2, 3)) / (2 * self.sigma_max ** 2)\n",
    "\n",
    "  def discretize(self, x, t):\n",
    "    \"\"\"SMLD(NCSN) discretization.\"\"\"\n",
    "    timestep = (t * (self.N - 1) / self.T).long()\n",
    "    sigma = self.discrete_sigmas.to(t.device)[timestep]\n",
    "    adjacent_sigma = torch.where(timestep == 0, torch.zeros_like(t),\n",
    "                                 self.discrete_sigmas[timestep - 1].to(t.device))\n",
    "    f = torch.zeros_like(x)\n",
    "    G = torch.sqrt(sigma ** 2 - adjacent_sigma ** 2)\n",
    "    return f, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fe569-0c78-4c75-abd4-904e5b1e7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "ckpt_filename = \"ckpt.pth\"\n",
    "sde = VESDE()\n",
    "sampling_eps = 1e-5\n",
    "batch_size = 8\n",
    "random_seed = 0 \n",
    "img_size = 512\n",
    "channels = 3\n",
    "shape = (batch_size, channels, img_size, img_size)\n",
    "snr = 0.16 \n",
    "n_steps =  1\n",
    "probability_flow = False\n",
    "lr=1e-4 \n",
    "\n",
    "optimizer = Adam(score_model.parameters(), lr=lr)\n",
    "\n",
    "config.training.batch_size = batch_size\n",
    "config.eval.batch_size = batch_size\n",
    "\n",
    "\n",
    "sigmas = mutils.get_sigmas(config)\n",
    "scaler = datasets.get_data_scaler(config)\n",
    "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
    "score_model = mutils.create_model(config)\n",
    "\n",
    "ema = ExponentialMovingAverage(score_model.parameters(),\n",
    "                               decay=config.model.ema_rate)\n",
    "state = dict(step=0, optimizer=optimizer,\n",
    "             model=score_model, ema=ema)\n",
    "\n",
    "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
    "ema.copy_to(score_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbef2c-389b-4a28-a672-b63f5d2450b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor = ReverseDiffusionPredictor #@param [\"EulerMaruyamaPredictor\", \"AncestralSamplingPredictor\", \"ReverseDiffusionPredictor\", \"None\"] {\"type\": \"raw\"}\n",
    "corrector = LangevinCorrector #@param [\"LangevinCorrector\", \"AnnealedLangevinDynamics\", \"None\"] {\"type\": \"raw\"}\n",
    "sampling_fn = sampling.get_pc_sampler(sde, shape, predictor, corrector,\n",
    "                                      inverse_scaler, snr, n_steps=n_steps,\n",
    "                                      probability_flow=probability_flow,\n",
    "                                      continuous=config.training.continuous,\n",
    "                                      eps=sampling_eps, device=device)\n",
    "\n",
    "x, n = sampling_fn(score_model)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
